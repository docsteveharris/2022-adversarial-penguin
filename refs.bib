
@article{bartlett,
	title = {Classiﬁcation with a {Reject} {Option} using a {Hinge} {Loss}},
	volume = {9},
	url = {https://www.jmlr.org/papers/volume9/bartlett08a/bartlett08a.pdf},
	abstract = {We consider the problem of binary classiﬁcation where the classiﬁer can, for a particular cost, choose not to classify an observation. Just as in the conventional classiﬁcation problem, minimization of the sample average of the cost is a difﬁcult optimization problem. As an alternative, we propose the optimization of a certain convex loss function φ, analogous to the hinge loss used in support vector machines (SVMs). Its convexity ensures that the sample average of this surrogate loss can be efﬁciently minimized. We study its statistical properties. We show that minimizing the expected surrogate loss—the φ-risk—also minimizes the risk. We also study the rate at which the φ-risk approaches its minimum value. We show that fast rates are possible when the conditional probability P(Y = 1{\textbar}X) is unlikely to be close to certain critical values.},
	number = {8},
	urldate = {2022-07-02},
	journal = {Journal of Machine Learning Research},
	author = {Bartlett, Peter L and Wegkamp, Marten H},
	year = {2008},
	pages = {18},
}

@article{bunz2022a,
	title = {The {AI} doctor will see you now: assessing the framing of {AI} in news coverage},
	volume = {37},
	issn = {0951-5666, 1435-5655},
	shorttitle = {The {AI} doctor will see you now},
	url = {https://link.springer.com/10.1007/s00146-021-01145-9},
	doi = {10.1007/s00146-021-01145-9},
	abstract = {One of the sectors for which Artificial Intelligence applications have been considered as exceptionally promising is the healthcare sector. As a public-facing sector, the introduction of AI applications has been subject to extended news coverage. This article conducts a quantitative and qualitative data analysis of English news media articles covering AI systems that allow the automation of tasks that so far needed to be done by a medical expert such as a doctor or a nurse thereby redistributing their agency. We investigated in this article one particular framing of AI systems and their agency: the framing that positions AI systems as (1a) replacing and (1b) outperforming the human medical expert, and in which (2) AI systems are personified and/or addressed as a person. The analysis of our data set consisting of 365 articles written between the years 1980 and 2019 will show that there is a tendency to present AI systems as outperforming human expertise. These findings are important given the central role of news coverage in explaining AI and given the fact that the popular frame of ‘outperforming’ might place AI systems above critique and concern including the Hippocratic oath. Our data also showed that the addressing of an AI system as a person is a trend that has been advanced only recently and is a new development in the public discourse about AI.},
	number = {1},
	urldate = {2022-05-07},
	journal = {AI \& SOCIETY},
	author = {Bunz, Mercedes and Braghieri, Marco},
	month = mar,
	year = {2022},
	keywords = {Agency, Artificial intelligence, Framing, Healthcare, Media coverage, Medicine, ai, hype, ml4h, press},
	pages = {9--22},
}

@misc{krishna2022a,
	title = {The {Disagreement} {Problem} in {Explainable} {Machine} {Learning}: {A} {Practitioner}'s {Perspective}},
	shorttitle = {The {Disagreement} {Problem} in {Explainable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2202.01602},
	abstract = {As various post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to develop a deeper understanding of if and when the explanations output by these methods disagree with each other, and how such disagreements are resolved in practice. However, there is little to no research that provides answers to these critical questions. In this work, we introduce and study the disagreement problem in explainable machine learning. More specifically, we formalize the notion of disagreement between explanations, analyze how often such disagreements occur in practice, and how do practitioners resolve these disagreements. To this end, we first conduct interviews with data scientists to understand what constitutes disagreement between explanations generated by different methods for the same model prediction, and introduce a novel quantitative framework to formalize this understanding. We then leverage this framework to carry out a rigorous empirical analysis with four real-world datasets, six state-of-the-art post hoc explanation methods, and eight different predictive models, to measure the extent of disagreement between the explanations generated by various popular explanation methods. In addition, we carry out an online user study with data scientists to understand how they resolve the aforementioned disagreements. Our results indicate that state-of-the-art explanation methods often disagree in terms of the explanations they output. Our findings also underscore the importance of developing principled evaluation metrics that enable practitioners to effectively compare explanations.},
	urldate = {2022-06-29},
	collaborator = {Krishna, Satyapriya and Han, Tessa and Gu, Alex and Pombra, Javin and Jabbari, Shahin and Wu, Steven and Lakkaraju, Himabindu},
	month = feb,
	year = {2022},
	note = {arXiv:2202.01602 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{liley2021,
	title = {Model updating after interventions paradoxically introduces bias},
	url = {http://proceedings.mlr.press/v130/liley21a.html},
	abstract = {Abstract Machine learning is increasingly being used to generate prediction models for use in a number of real-world settings, from credit risk assessment to clinical decision support. Recent discussions have highlighted potential problems in the updating of a predictive score for a binary outcome when an existing predictive score forms part of the standard workflow, driving interventions. In this setting, the existing score induces an additional causative pathway which leads to miscalibration when the original score is replaced. We propose a …},
	booktitle = {Proceedings of {Machine} {Learning} {Research}},
	author = {Liley, James and Emerson, Samuel and Mateen, Bilal and Vallejos, Catalina and Aslett, Louis and Vollmer, Sebastian},
	year = {2021},
	pages = {3916--3924},
}

@article{elyaniv2010,
	title = {On the {Foundations} of {Noise}-free {Selective} {Classiﬁcation}},
	volume = {11},
	abstract = {We consider selective classiﬁcation, a term we adopt here to refer to ‘classiﬁcation with a reject option.’ The essence in selective classiﬁcation is to trade-off classiﬁer coverage for higher accuracy. We term this trade-off the risk-coverage (RC) trade-off. Our main objective is to characterize this trade-off and to construct algorithms that can optimally or near optimally achieve the best possible trade-offs in a controlled manner. For noise-free models we present in this paper a thorough analysis of selective classiﬁcation including characterizations of RC trade-offs in various interesting settings.},
	language = {en},
	number = {5},
	journal = {Journal of Machine Learning Research},
	author = {El-Yaniv, Ran and Wiener, Yair},
	year = {2010},
	pages = {37},
}

@article{london2018a,
	title = {Learning health systems, clinical equipoise and the ethics of response adaptive randomisation},
	volume = {44},
	issn = {0306-6800, 1473-4257},
	url = {https://jme.bmj.com/lookup/doi/10.1136/medethics-2017-104549},
	doi = {10.1136/medethics-2017-104549},
	abstract = {To give substance to the rhetoric of ’learning health systems’, a variety of novel trial designs are being explored to more seamlessly integrate research with medical practice, reduce study duration and reduce the number of participants allocated to ineffective interventions. Many of these designs rely on response adaptive randomisation (RAR). However, critics charge that RAR is unethical on the grounds that it violates the principle of equipoise. In this paper, I reconstruct critiques of RAR as holding that it is inconsistent with ﬁve important ethical principles. I then argue that these criticisms rest on a faulty view of equipoise encouraged by the idea that a RAR study models the beliefs of a single rational agent about the relative merits of the interventions being studied. I outline a view in which RAR models an idealised health system in which diverse communities of fully informed experts shrink or grow as their constituent members update their expert opinions in light of reliable medical evidence. I show how a proper understanding of clinical equipoise can reconcile this conception of RAR with these ﬁve ethical principles. This analysis removes an in-principle objection to RAR and sheds important light on the relationship between clinical equipoise and transient diversity in the scientiﬁc community.},
	language = {en},
	number = {6},
	urldate = {2022-06-27},
	journal = {Journal of Medical Ethics},
	author = {London, Alex John},
	month = jun,
	year = {2018},
	pages = {409--415},
}

@article{turakhia2019a,
	title = {Rationale and design of a large-scale, app-based study to identify cardiac arrhythmias using a smartwatch: {The} {Apple} {Heart} {Study}},
	volume = {207},
	issn = {00028703},
	shorttitle = {Rationale and design of a large-scale, app-based study to identify cardiac arrhythmias using a smartwatch},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0002870318302710},
	doi = {10.1016/j.ahj.2018.09.002},
	abstract = {Background—Smartwatch and fitness band wearable consumer electronics can passively measure pulse rate from the wrist using photoplethysmography (PPG). Identification of pulse irregularity or variability from these data has the potential to identify atrial fibrillation or atrial flutter (AF, collectively). The rapidly expanding consumer base of these devices allows for detection of undiagnosed AF at scale.},
	language = {en},
	urldate = {2022-05-07},
	journal = {American Heart Journal},
	author = {Turakhia, Mintu P. and Desai, Manisha and Hedlin, Haley and Rajmane, Amol and Talati, Nisha and Ferris, Todd and Desai, Sumbul and Nag, Divya and Patel, Mithun and Kowey, Peter and Rumsfeld, John S. and Russo, Andrea M. and Hills, Mellanie True and Granger, Christopher B. and Mahaffey, Kenneth W. and Perez, Marco V.},
	month = jan,
	year = {2019},
	pages = {66--75},
}

@article{lin2019a,
	title = {Diagnostic efficacy and therapeutic decision-making capacity of an artificial intelligence platform for childhood cataracts in eye clinics: {A} multicentre randomized controlled trial.},
	volume = {9},
	journal = {EClinicalMedicine},
	author = {Lin, H and Li, R and Liu, Z and Chen, J and Yang, Y and Chen, H and Lin, Z and Lai, W and Long, E and Wu, X and Lin, D and Zhu, Y and Chen, C and Wu, D and Yu, T and Cao, Q and Li, X and Li, J and Li, W and Wang, J and Yang, M and Hu, H and Zhang, L and Yu, Y and Chen, X and Hu, J and Zhu, K and Jiang, S and Huang, Y and Tan, G and Huang, J and Lin, X and Zhang, X and Luo, L and Liu, Y and Liu, X and Cheng, B and Zheng, D and Wu, M and Chen, W and Liu, Y},
	year = {2019},
	pages = {52--59},
}

@article{sutton2020,
	title = {An overview of clinical decision support systems: benefits, risks, and strategies for success},
	volume = {3},
	copyright = {2020 The Author(s)},
	issn = {2398-6352},
	shorttitle = {An overview of clinical decision support systems},
	url = {https://www.nature.com/articles/s41746-020-0221-y},
	doi = {10.1038/s41746-020-0221-y},
	abstract = {Computerized clinical decision support systems, or CDSS, represent a paradigm shift in healthcare today. CDSS are used to augment clinicians in their complex decision-making processes. Since their first use in the 1980s, CDSS have seen a rapid evolution. They are now commonly administered through electronic medical records and other computerized clinical workflows, which has been facilitated by increasing global adoption of electronic medical records with advanced capabilities. Despite these advances, there remain unknowns regarding the effect CDSS have on the providers who use them, patient outcomes, and costs. There have been numerous published examples in the past decade(s) of CDSS success stories, but notable setbacks have also shown us that CDSS are not without risks. In this paper, we provide a state-of-the-art overview on the use of clinical decision support systems in medicine, including the different types, current use cases with proven efficacy, common pitfalls, and potential harms. We conclude with evidence-based recommendations for minimizing risk in CDSS design, implementation, evaluation, and maintenance.},
	number = {1},
	urldate = {2022-05-08},
	journal = {npj Digital Medicine},
	author = {Sutton, Reed T. and Pincock, David and Baumgart, Daniel C. and Sadowski, Daniel C. and Fedorak, Richard N. and Kroeker, Karen I.},
	month = feb,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Diagnosis, Drug regulation, Health services, Medical imaging},
	pages = {1--10},
}

@article{davis2020,
	title = {Detection of calibration drift in clinical prediction models to inform model updating},
	volume = {112},
	issn = {15320464},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046420302392},
	doi = {10.1016/j.jbi.2020.103611},
	urldate = {2022-06-24},
	journal = {Journal of Biomedical Informatics},
	author = {Davis, Sharon E. and Greevy, Robert A. and Lasko, Thomas A. and Walsh, Colin G. and Matheny, Michael E.},
	month = dec,
	year = {2020},
	pages = {103611},
}

@article{chow1970,
	title = {On optimum recognition error and reject tradeoff},
	volume = {16},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1054406/},
	doi = {10.1109/TIT.1970.1054406},
	number = {1},
	urldate = {2022-06-24},
	journal = {IEEE Transactions on Information Theory},
	author = {Chow, C.},
	month = jan,
	year = {1970},
	pages = {41--46},
}

@article{corrigan-curay2018a,
	title = {Real-{World} {Evidence} and {Real}-{World} {Data} for {Evaluating} {Drug} {Safety} and {Effectiveness}},
	volume = {320},
	issn = {0098-7484},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2018.10136},
	doi = {10.1001/jama.2018.10136},
	number = {9},
	urldate = {2022-05-07},
	journal = {JAMA},
	author = {Corrigan-Curay, Jacqueline and Sacks, Leonard and Woodcock, Janet},
	month = sep,
	year = {2018},
	pages = {867},
}

@article{lee2020,
	title = {Clinical applications of continual learning machine learning},
	volume = {2},
	issn = {2589-7500},
	url = {https://www.thelancet.com/journals/landig/article/PIIS2589-7500(20)30102-3/fulltext},
	doi = {10.1016/S2589-7500(20)30102-3},
	number = {6},
	urldate = {2022-05-09},
	journal = {The Lancet Digital Health},
	author = {Lee, Cecilia S. and Lee, Aaron Y.},
	month = jun,
	year = {2020},
	pmid = {33328120},
	note = {Publisher: Elsevier},
	pages = {e279--e281},
}

@article{davis2017,
	title = {Calibration drift in regression and machine learning models for acute kidney injury},
	volume = {24},
	issn = {1067-5027, 1527-974X},
	url = {https://academic.oup.com/jamia/article/24/6/1052/3096776},
	doi = {10.1093/jamia/ocx030},
	abstract = {Abstract
            
              Objective
              Predictive analytics create opportunities to incorporate personalized risk estimates into clinical decision support. Models must be well calibrated to support decision-making, yet calibration deteriorates over time. This study explored the influence of modeling methods on performance drift and connected observed drift with data shifts in the patient population.
            
            
              Materials and Methods
              Using 2003 admissions to Department of Veterans Affairs hospitals nationwide, we developed 7 parallel models for hospital-acquired acute kidney injury using common regression and machine learning methods, validating each over 9 subsequent years.
            
            
              Results
              Discrimination was maintained for all models. Calibration declined as all models increasingly overpredicted risk. However, the random forest and neural network models maintained calibration across ranges of probability, capturing more admissions than did the regression models. The magnitude of overprediction increased over time for the regression models while remaining stable and small for the machine learning models. Changes in the rate of acute kidney injury were strongly linked to increasing overprediction, while changes in predictor-outcome associations corresponded with diverging patterns of calibration drift across methods.
            
            
              Conclusions
              Efficient and effective updating protocols will be essential for maintaining accuracy of, user confidence in, and safety of personalized risk predictions to support decision-making. Model updating protocols should be tailored to account for variations in calibration drift across methods and respond to periods of rapid performance drift rather than be limited to regularly scheduled annual or biannual intervals.},
	number = {6},
	urldate = {2022-06-24},
	journal = {Journal of the American Medical Informatics Association},
	author = {Davis, Sharon E and Lasko, Thomas A and Chen, Guanhua and Siew, Edward D and Matheny, Michael E},
	month = nov,
	year = {2017},
	pages = {1052--1061},
}

@article{feng2021,
	title = {Approval policies for modifications to machine learning‐based software as a medical device: {A} study of bio‐creep},
	volume = {77},
	issn = {0006-341X, 1541-0420},
	shorttitle = {Approval policies for modifications to machine learning‐based software as a medical device},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/biom.13379},
	doi = {10.1111/biom.13379},
	abstract = {Successful deployment of machine learning algorithms in healthcare requires careful assessments of their performance and safety. To date, the FDA approves locked algorithms prior to marketing and requires future updates to undergo separate premarket reviews. However, this negates a key feature of machine learning—the ability to learn from a growing dataset and improve over time. This paper frames the design of an approval policy, which we refer to as an automatic algorithmic change protocol (aACP), as an online hypothesis testing problem. As this process has obvious analogy with noninferiority testing of new drugs, we investigate how repeated testing and adoption of modifications might lead to gradual deterioration in prediction accuracy, also known as “biocreep” in the drug development literature. We consider simple policies that one might consider but do not necessarily offer any error-rate guarantees, as well as policies that do provide error-rate control. For the latter, we define two online error-rates appropriate for this context: bad approval count (BAC) and bad approval and benchmark ratios (BABR). We control these rates in the simple setting of a constant population and data source using policies aACP-BAC and aACP-BABR, which combine alpha-investing, group-sequential, and gate-keeping methods. In simulation studies, bio-creep regularly occurred when using policies with no error-rate guarantees, whereas aACP-BAC and aACP-BABR controlled the rate of bio-creep without substantially impacting our ability to approve beneficial modifications.},
	number = {1},
	urldate = {2022-06-24},
	journal = {Biometrics},
	author = {Feng, Jean and Emerson, Scott and Simon, Noah},
	month = mar,
	year = {2021},
	pages = {31--44},
}

@article{shah2019,
	title = {Making {Machine} {Learning} {Models} {Clinically} {Useful}},
	volume = {322},
	issn = {0098-7484},
	url = {https://jamanetwork.com/journals/jama/fullarticle/2748179},
	doi = {10.1001/jama.2019.10306},
	number = {14},
	urldate = {2022-05-07},
	journal = {JAMA},
	author = {Shah, Nigam H. and Milstein, Arnold and Bagley, PhD, Steven C.},
	month = oct,
	year = {2019},
	pages = {1351},
}

@article{feng2022,
	title = {Clinical artificial intelligence quality improvement: towards continual monitoring and updating of {AI} algorithms in healthcare},
	volume = {5},
	issn = {2398-6352},
	shorttitle = {Clinical artificial intelligence quality improvement},
	url = {https://www.nature.com/articles/s41746-022-00611-y},
	doi = {10.1038/s41746-022-00611-y},
	abstract = {Abstract
            Machine learning (ML) and artificial intelligence (AI) algorithms have the potential to derive insights from clinical data and improve patient outcomes. However, these highly complex systems are sensitive to changes in the environment and liable to performance decay. Even after their successful integration into clinical practice, ML/AI algorithms should be continuously monitored and updated to ensure their long-term safety and effectiveness. To bring AI into maturity in clinical care, we advocate for the creation of hospital units responsible for quality assurance and improvement of these algorithms, which we refer to as “AI-QI” units. We discuss how tools that have long been used in hospital quality assurance and quality improvement can be adapted to monitor static ML algorithms. On the other hand, procedures for continual model updating are still nascent. We highlight key considerations when choosing between existing methods and opportunities for methodological innovation.},
	number = {1},
	urldate = {2022-06-24},
	journal = {npj Digital Medicine},
	author = {Feng, Jean and Phillips, Rachael V. and Malenica, Ivana and Bishara, Andrew and Hubbard, Alan E. and Celi, Leo A. and Pirracchio, Romain},
	month = dec,
	year = {2022},
	pages = {66},
}

@article{the2021,
	title = {{DECIDE}-{AI}: new reporting guidelines to bridge the development-to-implementation gap in clinical artificial intelligence},
	volume = {27},
	issn = {1078-8956, 1546-170X},
	shorttitle = {{DECIDE}-{AI}},
	url = {http://www.nature.com/articles/s41591-021-01229-5},
	doi = {10.1038/s41591-021-01229-5},
	number = {2},
	urldate = {2022-05-07},
	journal = {Nature Medicine},
	author = {{The DECIDE-AI Steering Group}},
	month = feb,
	year = {2021},
	pages = {186--187},
}

@article{woolf2008b,
	title = {The {Meaning} of {Translational} {Research} and {Why} {It} {Matters}},
	volume = {299},
	issn = {0098-7484},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2007.26},
	doi = {10.1001/jama.2007.26},
	number = {2},
	urldate = {2022-05-07},
	journal = {JAMA},
	author = {Woolf, Steven H.},
	month = jan,
	year = {2008},
}

@misc{2022b,
	title = {{DevOps}},
	url = {https://en.wikipedia.org/wiki/DevOps},
	urldate = {2022-07-01},
	year = {2022},
}

@article{burton2015,
	title = {Data {Safe} {Havens} in health research and healthcare},
	volume = {31},
	issn = {1367-4811},
	doi = {10.1093/bioinformatics/btv279},
	abstract = {MOTIVATION: The data that put the 'evidence' into 'evidence-based medicine' are central to developments in public health, primary and hospital care. A fundamental challenge is to site such data in repositories that can easily be accessed under appropriate technical and governance controls which are effectively audited and are viewed as trustworthy by diverse stakeholders. This demands socio-technical solutions that may easily become enmeshed in protracted debate and controversy as they encounter the norms, values, expectations and concerns of diverse stakeholders. In this context, the development of what are called 'Data Safe Havens' has been crucial. Unfortunately, the origins and evolution of the term have led to a range of different definitions being assumed by different groups. There is, however, an intuitively meaningful interpretation that is often assumed by those who have not previously encountered the term: a repository in which useful but potentially sensitive data may be kept securely under governance and informatics systems that are fit-for-purpose and appropriately tailored to the nature of the data being maintained, and may be accessed and utilized by legitimate users undertaking work and research contributing to biomedicine, health and/or to ongoing development of healthcare systems.
RESULTS: This review explores a fundamental question: 'what are the specific criteria that ought reasonably to be met by a data repository if it is to be seen as consistent with this interpretation and viewed as worthy of being accorded the status of 'Data Safe Haven' by key stakeholders'? We propose 12 such criteria.
CONTACT: paul.burton@bristol.ac.uk.},
	number = {20},
	journal = {Bioinformatics (Oxford, England)},
	author = {Burton, Paul R. and Murtagh, Madeleine J. and Boyd, Andy and Williams, James B. and Dove, Edward S. and Wallace, Susan E. and Tassé, Anne-Marie and Little, Julian and Chisholm, Rex L. and Gaye, Amadou and Hveem, Kristian and Brookes, Anthony J. and Goodwin, Pat and Fistein, Jon and Bobrow, Martin and Knoppers, Bartha M.},
	month = oct,
	year = {2015},
	pmid = {26112289},
	pmcid = {PMC4595892},
	keywords = {Access to Information, Biomedical Research, Confidentiality, Delivery of Health Care, Humans, Research},
	pages = {3241--3248},
}

@misc{nhsx2022,
	title = {The national strategy for {AI} in health and social care},
	url = {https://www.nhsx.nhs.uk/ai-lab/ai-lab-programmes/the-national-strategy-for-ai-in-health-and-social-care/},
	urldate = {2022-07-01},
	year = {2022},
}

@misc{2021f,
	title = {Digital future index 2021-2022},
	url = {https://www.digicatapult.org.uk/news-and-insights/publications/post/digital-future-index-2021-2022/},
	urldate = {2022-07-01},
	year = {2021},
}

@article{morse2020a,
	title = {Estimate the hidden deployment cost of predictive models to improve patient care.},
	volume = {26},
	number = {1},
	journal = {Nature medicine},
	author = {Morse, KE and Bagley, SC and Shah, NH},
	year = {2020},
	pages = {18--19},
}

@article{komorowski2018,
	title = {The {Artificial} {Intelligence} {Clinician} learns optimal treatment strategies for sepsis in intensive care},
	volume = {24},
	issn = {1078-8956, 1546-170X},
	url = {http://www.nature.com/articles/s41591-018-0213-5},
	doi = {10.1038/s41591-018-0213-5},
	number = {11},
	urldate = {2022-05-07},
	journal = {Nature Medicine},
	author = {Komorowski, Matthieu and Celi, Leo A. and Badawi, Omar and Gordon, Anthony C. and Faisal, A. Aldo},
	month = nov,
	year = {2018},
	pages = {1716--1720},
}

@article{keane2018,
	title = {With an eye to {AI} and autonomous diagnosis},
	volume = {1},
	issn = {2398-6352},
	url = {http://www.nature.com/articles/s41746-018-0048-y},
	doi = {10.1038/s41746-018-0048-y},
	number = {1},
	urldate = {2022-05-08},
	journal = {npj Digital Medicine},
	author = {Keane, Pearse A. and Topol, Eric J.},
	month = dec,
	year = {2018},
	pages = {40, s41746--018--0048--y},
}

@article{everson2020,
	title = {Reconsidering hospital {EHR} adoption at the dawn of {HITECH}: implications of the reported 9\% adoption of a “basic” {EHR}},
	volume = {27},
	issn = {1527-974X},
	shorttitle = {Reconsidering hospital {EHR} adoption at the dawn of {HITECH}},
	url = {https://academic.oup.com/jamia/article/27/8/1198/5862603},
	doi = {10.1093/jamia/ocaa090},
	abstract = {Objective: In 2009, a prominent national report stated that 9\% of US hospitals had adopted a “basic” electronic health record (EHR) system. This statistic was widely cited and became a memetic anchor point for EHR adoption at the dawn of HITECH. However, its calculation relies on speciﬁc treatment of the data; alternative approaches may have led to a different sense of US hospitals’ EHR adoption and different subsequent public policy. Materials and Methods: We reanalyzed the 2008 American Heart Association Information Technology supplement and complementary sources to produce a range of estimates of EHR adoption. Estimates included the mean and median number of EHR functionalities adopted, ﬁgures derived from an item response theory-based approach, and alternative estimates from the published literature. We then plotted an alternative deﬁnition of national progress toward hospital EHR adoption from 2008 to 2018.
Results: By 2008, 73\% of hospitals had begun the transition to an EHR, and the majority of hospitals had adopted at least 6 of the 10 functionalities of a basic system. In the aggregate, national progress toward basic EHR adoption was 58\% complete, and, when accounting for measurement error, we estimate that 30\% of hospitals may have adopted a basic EHR. Discussion: The approach used to develop the 9\% ﬁgure resulted in an estimate at the extreme lower bound of what could be derived from the available data and likely did not reﬂect hospitals’ overall progress in EHR adoption.
Conclusion: The memetic 9\% ﬁgure shaped nationwide thinking and policy making about EHR adoption; alternative representations of the data may have led to different policy.},
	number = {8},
	urldate = {2022-05-07},
	journal = {Journal of the American Medical Informatics Association},
	author = {Everson, Jordan and Rubin, Joshua C and Friedman, Charles P},
	month = aug,
	year = {2020},
	pages = {1198--1205},
}

@article{zippel2021,
	title = {Rise of clinical studies in the field of machine learning: {A} review of data registered in {ClinicalTrials}.gov.},
	volume = {18},
	number = {10},
	journal = {International journal of environmental research and public health},
	author = {Zippel, C and Bohnet-Joschko, S},
	year = {2021},
	pages = {5072},
}

@article{yusop2017,
	title = {Reporting {Usability} {Defects}: {A} {Systematic} {Literature} {Review}},
	volume = {43},
	issn = {1939-3520},
	shorttitle = {Reporting {Usability} {Defects}},
	doi = {10.1109/TSE.2016.2638427},
	abstract = {Usability defects can be found either by formal usability evaluation methods or indirectly during system testing or usage. No matter how they are discovered, these defects must be tracked and reported. However, empirical studies indicate that usability defects are often not clearly and fully described. This study aims to identify the state of the art in reporting of usability defects in the software engineering and usability engineering literature. We conducted a systematic literature review of usability defect reporting drawing from both the usability and software engineering literature from January 2000 until March 2016. As a result, a total of 57 studies were identified, in which we classified the studies into three categories: reporting usability defect information, analysing usability defect data and key challenges. Out of these, 20 were software engineering studies and 37 were usability studies. The results of this systematic literature review show that usability defect reporting processes suffer from a number of limitations, including: mixed data, inconsistency of terms and values of usability defect data, and insufficient attributes to classify usability defects. We make a number of recommendations to improve usability defect reporting and management in software engineering.},
	number = {9},
	journal = {IEEE Transactions on Software Engineering},
	author = {Yusop, Nor Shahida Mohamad and Grundy, John and Vasa, Rajesh},
	month = sep,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Bibliographies, Human computer interaction, Software engineering, Systematic review, Systematics, Testing, Usability, test management, usability defect reporting, usability testing, user interface},
	pages = {848--867},
}

@article{wilson2021,
	title = {Electronic health record alerts for acute kidney injury: multicenter, randomized clinical trial},
	issn = {1756-1833},
	shorttitle = {Electronic health record alerts for acute kidney injury},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.m4786},
	doi = {10.1136/bmj.m4786},
	abstract = {Objective To determine whether electronic health record alerts for acute kidney injury would improve patient outcomes of mortality, dialysis, and progression of acute kidney injury. Design Double blinded, multicenter, parallel, randomized controlled trial. Setting Six hospitals (four teaching and two non-teaching) in the Yale New Haven Health System in Connecticut and Rhode Island, US, ranging from small community hospitals to large tertiary care centers. Participants 6030 adult inpatients with acute kidney injury, as defined by the Kidney Disease: Improving Global Outcomes (KDIGO) creatinine criteria. Interventions An electronic health record based “pop-up” alert for acute kidney injury with an associated acute kidney injury order set upon provider opening of the patient’s medical record. Main outcome measures A composite of progression of acute kidney injury, receipt of dialysis, or death within 14 days of randomization. Prespecified secondary outcomes included outcomes at each hospital and frequency of various care practices for acute kidney injury.},
	language = {en},
	urldate = {2022-05-08},
	journal = {BMJ},
	author = {Wilson, F Perry and Martin, Melissa and Yamamoto, Yu and Partridge, Caitlin and Moreira, Erica and Arora, Tanima and Biswas, Aditya and Feldman, Harold and Garg, Amit X and Greenberg, Jason H and Hinchcliff, Monique and Latham, Stephen and Li, Fan and Lin, Haiqun and Mansour, Sherry G and Moledina, Dennis G and Palevsky, Paul M and Parikh, Chirag R and Simonov, Michael and Testani, Jeffrey and Ugwuowo, Ugochukwu},
	month = jan,
	year = {2021},
	pages = {m4786},
}

@article{wu2019,
	title = {Randomised controlled trial of {WISENSE}, a real-time quality improving system for monitoring blind spots during esophagogastroduodenoscopy.},
	volume = {68},
	number = {12},
	journal = {Gut},
	author = {Wu, L and Zhang, J and Zhou, W and An, P and Shen, L and Liu, J and Jiang, X and Huang, X and Mu, G and Wan, X and Lv, X and Gao, J and Cui, N and Hu, S and Chen, Y and Hu, X and Li, J and Chen, D and Gong, D and He, X and Ding, Q and Zhu, X and Li, S and Wei, X and Li, X and Wang, X and Zhou, J and Zhang, M and Yu, HG},
	year = {2019},
	pages = {2161--2169},
}

@article{wilson2022,
	title = {Learning from individualised variation for evidence generation within a learning health system},
	journal = {British Journal of Anaesthesia},
	author = {Wilson, Matthew G. and Asselbergs, Folkert W. and Harris, Steve K.},
	year = {2022},
}

@article{williamson2020,
	title = {Factors associated with {COVID}-19-related death using {OpenSAFELY}.},
	volume = {584},
	number = {7821},
	journal = {Nature},
	author = {Williamson, EJ and Walker, AJ and Bhaskaran, K and Bacon, S and Bates, C and Morton, CE and Curtis, HJ and Mehrkar, A and Evans, D and Inglesby, P and Cockburn, J and McDonald, HI and MacKenna, B and Tomlinson, L and Douglas, IJ and Rentsch, CT and Mathur, R and Wong, AYS and Grieve, R and Harrison, D and Forbes, H and Schultze, A and Croker, R and Parry, J and Hester, F and Harper, S and Perera, R and Evans, SJW and Smeeth, L and Goldacre, B},
	year = {2020},
	pages = {430--436},
}

@article{wang2019,
	title = {Real-time automatic detection system increases colonoscopic polyp and adenoma detection rates: a prospective randomised controlled study.},
	volume = {68},
	number = {10},
	journal = {Gut},
	author = {Wang, P and Berzin, TM and Glissen Brown, JR and Bharadwaj, S and Becq, A and Xiao, X and Liu, P and Li, L and Song, Y and Zhang, D and Li, Y and Xu, G and Tu, M and Liu, X},
	year = {2019},
	pages = {1813--1819},
}

@article{vilone2020,
	title = {Explainable artificial intelligence: a systematic review},
	journal = {arXiv preprint arXiv:2006.00093},
	author = {Vilone, G and Longo, L},
	year = {2020},
}

@article{vannorman2019,
	title = {Phase {II} {Trials} in {Drug} {Development} and {Adaptive} {Trial} {Design}},
	volume = {4},
	issn = {2452-302X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6609997/},
	doi = {10.1016/j.jacbts.2019.02.005},
	abstract = {Phase II clinical studies represent a critical point in determining drug costs, and phase II is a poor predictor of drug success: {\textgreater}30\% of drugs entering phase II studies fail to progress, and {\textgreater}58\% of drugs go on to fail in phase III. Adaptive clinical trial design has been proposed as a way to reduce the costs of phase II testing by providing earlier determination of futility and prediction of phase III success, reducing overall phase II and III trial sizes, and shortening overall drug development time. This review examines issues in phase II testing and adaptive trial design.},
	number = {3},
	urldate = {2022-05-09},
	journal = {JACC: Basic to Translational Science},
	author = {Van Norman, Gail A.},
	month = jun,
	year = {2019},
	pmid = {31312766},
	pmcid = {PMC6609997},
	pages = {428--437},
}

@article{turley2016,
	title = {Leveraging a statewide clinical data warehouse to expand boundaries of the learning health system},
	volume = {4},
	number = {1},
	journal = {eGEMs (Generating Evidence \& Methods to improve patient outcomes)},
	author = {Turley, Christine B.},
	year = {2016},
	pages = {25},
}

@incollection{tortorella2000,
	address = {Berlin, Heidelberg},
	title = {An {Optimal} {Reject} {Rule} for {Binary} {Classifiers}},
	volume = {1876},
	isbn = {978-3-540-67946-2 978-3-540-44522-7},
	url = {http://link.springer.com/10.1007/3-540-44522-6_63},
	urldate = {2022-06-29},
	booktitle = {Advances in {Pattern} {Recognition}},
	publisher = {Springer Berlin Heidelberg},
	author = {Tortorella, Francesco},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Ferri, Francesc J. and Iñesta, José M. and Amin, Adnan and Pudil, Pavel},
	year = {2000},
	doi = {10.1007/3-540-44522-6_63},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {611--620},
}

@article{titano2018,
	title = {Automated deep-neural-network surveillance of cranial images for acute neurologic events},
	volume = {24},
	issn = {1078-8956, 1546-170X},
	url = {http://www.nature.com/articles/s41591-018-0147-y},
	doi = {10.1038/s41591-018-0147-y},
	language = {en},
	number = {9},
	urldate = {2022-05-07},
	journal = {Nature Medicine},
	author = {Titano, Joseph J. and Badgeley, Marcus and Schefflein, Javin and Pain, Margaret and Su, Andres and Cai, Michael and Swinburne, Nathaniel and Zech, John and Kim, Jun and Bederson, Joshua and Mocco, J. and Drayer, Burton and Lehar, Joseph and Cho, Samuel and Costa, Anthony and Oermann, Eric K.},
	month = sep,
	year = {2018},
	pages = {1337--1341},
}

@incollection{steinert2010,
	title = {Scrutinizing gartner’s hype cycle approach},
	booktitle = {Scrutinizing gartner’s hype cycle approach},
	author = {Steinert, Martin and Leifer, Larry},
	year = {2010},
	pages = {1--13},
}

@article{sendak2020,
	title = {Real-world integration of a sepsis deep learning technology into routine clinical care: {Implementation} study.},
	volume = {8},
	number = {7},
	journal = {JMIR medical informatics},
	author = {Sendak, MP and Ratliff, W and Sarro, D and Alderton, E and Futoma, J and Gao, M and Nichols, M and Revoir, M and Yashar, F and Miller, C and Kester, K and Sandhu, S and Corey, K and Brajer, N and Tan, C and Lin, A and Brown, T and Engelbosch, S and Anstrom, K and Elish, MC and Heller, K and Donohoe, R and Theiling, J and Poon, E and Balu, S and Bedoya, A and O’Brien, C},
	year = {2020},
	pages = {e15182},
}

@article{sculley2015,
	title = {Hidden technical debt in machine learning systems},
	volume = {28},
	journal = {Advances in neural information processing systems},
	author = {Sculley, David and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Francois and Dennison, Dan},
	year = {2015},
	pages = {2503--2511},
}

@article{tomasev2019,
	title = {A clinically applicable approach to continuous prediction of future acute kidney injury.},
	volume = {572},
	number = {7767},
	journal = {Nature},
	author = {Tomašev, N and Glorot, X and Rae, JW and Zielinski, M and Askham, H and Saraiva, A and Mottram, A and Meyer, C and Ravuri, S and Protsyuk, I and Connell, A and Hughes, CO and Karthikesalingam, A and Cornebise, J and Montgomery, H and Rees, G and Laing, C and Baker, CR and Peterson, K and Reeves, R and Hassabis, D and King, D and Suleyman, M and Back, T and Nielson, C and Ledsam, JR and Mohamed, S},
	year = {2019},
	pages = {116--119},
}

@article{sayres2019,
	title = {Using a {Deep} {Learning} {Algorithm} and {Integrated} {Gradients} {Explanation} to {Assist} {Grading} for {Diabetic} {Retinopathy}},
	volume = {126},
	issn = {01616420},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0161642018315756},
	doi = {10.1016/j.ophtha.2018.11.016},
	abstract = {Purpose: To understand the impact of deep learning diabetic retinopathy (DR) algorithms on physician readers in computer-assisted settings. Design: Evaluation of diagnostic technology. Participants: One thousand seven hundred ninety-six retinal fundus images from 1612 diabetic patients.
Methods: Ten ophthalmologists (5 general ophthalmologists, 4 retina specialists, 1 retina fellow) read images for DR severity based on the International Clinical Diabetic Retinopathy disease severity scale in each of 3 conditions: unassisted, grades only, or grades plus heatmap. Grades-only assistance comprised a histogram of DR predictions (grades) from a trained deep-learning model. For grades plus heatmap, we additionally showed explanatory heatmaps. Main Outcome Measures: For each experiment arm, we computed sensitivity and speciﬁcity of each reader and the algorithm for different levels of DR severity against an adjudicated reference standard. We also measured accuracy (exact 5-class level agreement and Cohen’s quadratically weighted k), reader-reported conﬁdence (5-point Likert scale), and grading time.
Results: Readers graded more accurately with model assistance than without for the grades-only condition (P {\textless} 0.001). Grades plus heatmaps improved accuracy for patients with DR (P {\textless} 0.001), but reduced accuracy for patients without DR (P ¼ 0.006). Both forms of assistance increased readers’ sensitivity moderate-or-worse DR: unassisted: mean, 79.4\% [95\% conﬁdence interval (CI), 72.3\%e86.5\%]; grades only: mean, 87.5\% [95\% CI, 85.1\%e89.9\%]; grades plus heatmap: mean, 88.7\% [95\% CI, 84.9\%e92.5\%] without a corresponding drop in speciﬁcity (unassisted: mean, 96.6\% [95\% CI, 95.9\%e97.4\%]; grades only: mean, 96.1\% [95\% CI, 95.5\%e 96.7\%]; grades plus heatmap: mean, 95.5\% [95\% CI, 94.8\%e96.1\%]). Algorithmic assistance increased the accuracy of retina specialists above that of the unassisted reader or model alone; and increased grading conﬁdence and grading time across all readers. For most cases, grades plus heatmap was only as effective as grades only. Over the course of the experiment, grading time decreased across all conditions, although most sharply for grades plus heatmap.
Conclusions: Deep learning algorithms can improve the accuracy of, and conﬁdence in, DR diagnosis in an assisted read setting. They also may increase grading time, although these effects may be ameliorated with experience. Ophthalmology 2019;126:552-564 ª 2018 by the American Academy of Ophthalmology. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).},
	language = {en},
	number = {4},
	urldate = {2022-05-07},
	journal = {Ophthalmology},
	author = {Sayres, Rory and Taly, Ankur and Rahimy, Ehsan and Blumer, Katy and Coz, David and Hammel, Naama and Krause, Jonathan and Narayanaswamy, Arunachalam and Rastegar, Zahra and Wu, Derek and Xu, Shawn and Barb, Scott and Joseph, Anthony and Shumski, Michael and Smith, Jesse and Sood, Arjun B. and Corrado, Greg S. and Peng, Lily and Webster, Dale R.},
	month = apr,
	year = {2019},
	pages = {552--564},
}

@article{santanna2021,
	title = {Nudging healthcare professionals in clinical settings: a scoping review of the literature.},
	volume = {21},
	number = {1},
	journal = {BMC health services research},
	author = {Sant’Anna, A and Vilhelmsson, A and Wolf, A},
	year = {2021},
	pages = {543},
}

@article{powles2017,
	title = {Google {DeepMind} and healthcare in an age of algorithms},
	volume = {7},
	issn = {2190-7188, 2190-7196},
	url = {http://link.springer.com/10.1007/s12553-017-0179-1},
	doi = {10.1007/s12553-017-0179-1},
	abstract = {Data-driven tools and techniques, particularly machine learning methods that underpin artificial intelligence, offer promise in improving healthcare systems and services. One of the companies aspiring to pioneer these advances is DeepMind Technologies Limited, a wholly-owned subsidiary of the Google conglomerate, Alphabet Inc. In 2016, DeepMind announced its first major health project: a collaboration with the Royal Free London NHS Foundation Trust, to assist in the management of acute kidney injury. Initially received with great enthusiasm, the collaboration has suffered from a lack of clarity and openness, with issues of privacy and power emerging as potent challenges as the project has unfolded. Taking the DeepMind-Royal Free case study as its pivot, this article draws a number of lessons on the transfer of population-derived datasets to large private prospectors, identifying critical questions for policy-makers, industry and individuals as healthcare moves into an algorithmic age.},
	language = {en},
	number = {4},
	urldate = {2022-05-07},
	journal = {Health and Technology},
	author = {Powles, Julia and Hodson, Hal},
	month = dec,
	year = {2017},
	pages = {351--367},
}

@article{phansalkar2013,
	title = {Drug-drug interactions that should be non-interruptive in order to reduce alert fatigue in electronic health records.},
	volume = {20},
	number = {3},
	journal = {Journal of the American Medical Informatics Association : JAMIA},
	author = {Phansalkar, S and van der Sijs, H and Tucker, AD and Desai, AA and Bell, DS and Teich, JM and Middleton, B and Bates, DW},
	year = {2013},
	pages = {489--493},
}

@article{park2022,
	title = {Retrospective review of missed cancer detection and its mammography findings with artificial-intelligence-based, computer-aided diagnosis.},
	volume = {12},
	number = {2},
	journal = {Diagnostics (Basel, Switzerland)},
	author = {Park, GE and Kang, BJ and Kim, SH and Lee, J},
	year = {2022},
	pages = {387},
}

@article{scobie2020,
	title = {Implementing learning health systems in the {UK} {NHS}: {Policy} actions to improve collaboration and transparency and support innovation and better use of analytics},
	volume = {4},
	issn = {2379-6146},
	shorttitle = {Implementing learning health systems in the {UK} {NHS}},
	doi = {10.1002/lrh2.10209},
	abstract = {Learning health systems (LHS) use digital health and care data to improve care, shorten the timeframe of improvement projects, and ensure these are based on real-world data. In the United Kingdom, policymakers are depending on digital innovation, driven by better use of data about current health service performance, to enable service transformation and a more sustainable health system. This paper examines what would be needed to develop LHS in the United Kingdom, considering national policy implications and actions, which local organisations and health systems could take. The paper draws on a seminar attended by academics, policymakers, and practitioners, a brief literature review, and feedback from policy experts and National Health Service (NHS) stakeholders. Although there are examples of some aspects of LHS in the UK NHS, it is hard to find examples where there is a continuous cycle of improvement driven by information and where analysis of data and implementing improvements is part of usual ways of working. The seminar and literature identified a number of barriers. Incentives and capacity to develop LHS are limited, and requires a shift in analytic capacity from regulation and performance, to quality improvement and transformation. The balance in priority given to research compared with implementation also needs to change. Policy initiatives are underway which address some barriers, including building analytical capacity, developing infrastructure, and data standards. The NHS and research partners are investing in infrastructure which could support LHS, although clinical buy in is needed to bring about improvement or address operational challenges. We identify a number of opportunities for local NHS organisations and systems to make better use of health data, and for ways that national policy could promote the collaboration and greater use of analytics which underpin the LHS concept.},
	language = {eng},
	number = {1},
	journal = {Learning Health Systems},
	author = {Scobie, Sarah and Castle-Clarke, Sophie},
	year = {2020},
	pmid = {31989031},
	pmcid = {PMC6971118},
	keywords = {analytics, digital policy, implementation, innovation, learning health system, transparency},
	pages = {e10209},
}

@misc{ritchie2017,
	title = {The ‘{Five} {Safes}’: a framework for planning, designing and evaluating data access solutions},
	url = {https://uwe-repository.worktribe.com/OutputFile/880718},
	urldate = {2022-07-01},
	author = {Ritchie, Felix},
	year = {2017},
}

@article{razavian2020,
	title = {A validated, real-time prediction model for favorable outcomes in hospitalized {COVID}-19 patients},
	volume = {3},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-020-00343-x},
	doi = {10.1038/s41746-020-00343-x},
	abstract = {Abstract
            The COVID-19 pandemic has challenged front-line clinical decision-making, leading to numerous published prognostic tools. However, few models have been prospectively validated and none report implementation in practice. Here, we use 3345 retrospective and 474 prospective hospitalizations to develop and validate a parsimonious model to identify patients with favorable outcomes within 96 h of a prediction, based on real-time lab values, vital signs, and oxygen support variables. In retrospective and prospective validation, the model achieves high average precision (88.6\% 95\% CI: [88.4–88.7] and 90.8\% [90.8–90.8]) and discrimination (95.1\% [95.1–95.2] and 86.8\% [86.8–86.9]) respectively. We implemented and integrated the model into the EHR, achieving a positive predictive value of 93.3\% with 41\% sensitivity. Preliminary results suggest clinicians are adopting these scores into their clinical workflows.},
	language = {en},
	number = {1},
	urldate = {2022-07-01},
	journal = {npj Digital Medicine},
	author = {Razavian, Narges and Major, Vincent J. and Sudarshan, Mukund and Burk-Rafel, Jesse and Stella, Peter and Randhawa, Hardev and Bilaloglu, Seda and Chen, Ji and Nguy, Vuthy and Wang, Walter and Zhang, Hao and Reinstein, Ilan and Kudlowitz, David and Zenger, Cameron and Cao, Meng and Zhang, Ruina and Dogra, Siddhant and Harish, Keerthi B. and Bosworth, Brian and Francois, Fritz and Horwitz, Leora I. and Ranganath, Rajesh and Austrian, Jonathan and Aphinyanaphongs, Yindalon},
	month = dec,
	year = {2020},
	pages = {130},
}

@incollection{pizka2004,
	title = {Straightening spaghetti-code with refactoring},
	volume = {Software Engineering Research and Practice},
	booktitle = {Straightening spaghetti-code with refactoring},
	author = {Pizka, Markus},
	year = {2004},
	pages = {846--852},
}

@article{pepe2001,
	title = {Phases of {Biomarker} {Development} for {Early} {Detection} of {Cancer}},
	volume = {93},
	issn = {0027-8874},
	url = {https://doi.org/10.1093/jnci/93.14.1054},
	doi = {10.1093/jnci/93.14.1054},
	number = {14},
	urldate = {2022-06-27},
	journal = {JNCI: Journal of the National Cancer Institute},
	author = {Pepe, Margaret Sullivan and Etzioni, Ruth and Feng, Ziding and Potter, John D. and Thompson, Mary Lou and Thornquist, Mark and Winget, Marcy and Yasui, Yutaka},
	month = jul,
	year = {2001},
	pages = {1054--1061},
}

@article{mueller2019,
	title = {Explanation in human-{AI} systems: {A} literature meta-review, synopsis of key ideas and publications, and bibliography for explainable {AI}},
	journal = {arXiv preprint arXiv:1902.01876},
	author = {Mueller, ST and Hoffman, RR and Clancey, W and Emrey, A and Klein, G},
	year = {2019},
}

@article{muehlematter2021,
	title = {Approval of artificial intelligence and machine learning-based medical devices in the {USA} and {Europe} (2015-20): a comparative analysis.},
	volume = {3},
	journal = {The Lancet. Digital health},
	author = {Muehlematter, UJ and Daniore, P and Vokinger, KN},
	year = {2021},
	pages = {e195--e203},
}

@article{parisi2019,
	title = {Continual lifelong learning with neural networks: {A} review},
	volume = {113},
	journal = {Neural Networks},
	author = {Parisi, G and Kemker, R and Part, JL and Kanan, C and Wermter, S},
	year = {2019},
	pages = {54--71},
}

@article{mccoy2022,
	title = {Believing in black boxes: machine learning for healthcare does not need explainability to be evidence-based.},
	volume = {142},
	journal = {Journal of clinical epidemiology},
	author = {McCoy, LG and Brenna, CTA and Chen, SS and Vold, K and Das, S},
	year = {2022},
	pages = {252--257},
}

@article{long2017,
	title = {An artificial intelligence platform for the multihospital collaborative management of congenital cataracts},
	volume = {1},
	number = {2},
	journal = {Nature Biomedical Engineering},
	author = {Long, Erping and Lin, Haotian and Liu, Zhenzhen and Wu, Xiaohang and Wang, Liming and Jiang, Jiewei and An, Yingying and Lin, Zhuoling and Li, Xiaoyan and Chen, Jingjing and Li, Jing and Cao, Qianzhong and Wang, Dongni and Liu, Xiyang and Chen, Weirong and Liu, Yizhi},
	year = {2017},
}

@article{meyer2019,
	title = {Objecting to experiments that compare two unobjectionable policies or treatments},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1820701116},
	doi = {10.1073/pnas.1820701116},
	abstract = {Randomized experiments have enormous potential to improve human welfare in many domains, including healthcare, education, finance, and public policy. However, such “A/B tests” are often criticized on ethical grounds even as similar, untested interventions are implemented without objection. We find robust evidence across 16 studies of 5,873 participants from three diverse populations spanning nine domains—from healthcare to autonomous vehicle design to poverty reduction—that people frequently rate A/B tests designed to establish the comparative effectiveness of two policies or treatments as inappropriate even when universally implementing either A or B, untested, is seen as appropriate. This “A/B effect” is as strong among those with higher educational attainment and science literacy and among relevant professionals. It persists even when there is no reason to prefer A to B and even when recipients are treated unequally and randomly in all conditions (A, B, and A/B). Several remaining explanations for the effect—a belief that consent is required to impose a policy on half of a population but not on the entire population; an aversion to controlled but not to uncontrolled experiments; and a proxy form of the illusion of knowledge (according to which randomized evaluations are unnecessary because experts already do or should know “what works”)—appear to contribute to the effect, but none dominates or fully accounts for it. We conclude that rigorously evaluating policies or treatments via pragmatic randomized trials may provoke greater objection than simply implementing those same policies or treatments untested.},
	language = {en},
	number = {22},
	urldate = {2022-06-27},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Meyer, Michelle N. and Heck, Patrick R. and Holtzman, Geoffrey S. and Anderson, Stephen M. and Cai, William and Watts, Duncan J. and Chabris, Christopher F.},
	month = may,
	year = {2019},
	pages = {10723--10728},
}

@article{mcrae2015,
	title = {Facebook, {Airbnb}, {Uber}, and the unstoppable rise of the content non-generators},
	journal = {The Independent},
	author = {McRae, Hamish},
	year = {2015},
}

@article{main2010,
	title = {Computerised decision support systems in order communication for diagnostic, screening or monitoring test ordering: systematic reviews of the effects and cost-effectiveness of systems},
	volume = {14},
	issn = {1366-5278, 2046-4924},
	shorttitle = {Computerised decision support systems in order communication for diagnostic, screening or monitoring test ordering},
	url = {https://www.journalslibrary.nihr.ac.uk/hta/hta14480/},
	doi = {10.3310/hta14480},
	language = {en},
	number = {48},
	urldate = {2022-05-07},
	journal = {Health Technology Assessment},
	author = {Main, C and Moxham, T and Wyatt, J C and Kay, J and Anderson, R and Stein, K},
	month = oct,
	year = {2010},
}

@article{linardatos2020,
	title = {Explainable ai: {A} review of machine learning interpretability methods},
	volume = {23},
	number = {1},
	journal = {Entropy. An International and Interdisciplinary Journal of Entropy and Information Studies},
	author = {Linardatos, P and Papastefanopoulos, V and Kotsiantis, S},
	year = {2020},
}

@article{leibig2017,
	title = {Leveraging uncertainty information from deep neural networks for disease detection},
	volume = {7},
	number = {1},
	journal = {Scientific reports},
	author = {Leibig, C and Allken, V and Ayhan, MS and Berens, P and Wahl, S},
	year = {2017},
	pages = {1--14},
}

@article{lo2018,
	title = {Machine learning in chemoinformatics and drug discovery},
	volume = {23},
	issn = {1359-6446},
	url = {https://www.sciencedirect.com/science/article/pii/S1359644617304695},
	doi = {10.1016/j.drudis.2018.05.010},
	abstract = {Chemoinformatics is an established discipline focusing on extracting, processing and extrapolating meaningful data from chemical structures. With the rapid explosion of chemical ‘big’ data from HTS and combinatorial synthesis, machine learning has become an indispensable tool for drug designers to mine chemical information from large compound databases to design drugs with important biological properties. To process the chemical data, we first reviewed multiple processing layers in the chemoinformatics pipeline followed by the introduction of commonly used machine learning models in drug discovery and QSAR analysis. Here, we present basic principles and recent case studies to demonstrate the utility of machine learning techniques in chemoinformatics analyses; and we discuss limitations and future directions to guide further development in this evolving field.},
	language = {en},
	number = {8},
	urldate = {2022-05-09},
	journal = {Drug Discovery Today},
	author = {Lo, Yu-Chen and Rensi, Stefano E. and Torng, Wen and Altman, Russ B.},
	month = aug,
	year = {2018},
	pages = {1538--1546},
}

@article{king2022,
	title = {Machine learning for real-time aggregated prediction of hospital admission for emergency patients},
	journal = {medRxiv : the preprint server for health sciences},
	author = {King, Zella and Farrington, Joseph and Utley, Martin and Kung, Enoch and Elkhodair, Samer and Harris, Steve and Sekula, Richard and Li, Kezhi and Crowe, Sonya},
	year = {2022},
}

@incollection{john2021,
	title = {Towards {MLOps}: {A} framework and maturity model},
	volume = {2021 47th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)},
	booktitle = {Towards {MLOps}: {A} framework and maturity model},
	publisher = {IEEE},
	author = {John, Meenu Mary and Olsson, Helena Holmström and Bosch, Jan},
	year = {2021},
	pages = {1--8},
}

@article{horwitz2019,
	title = {Creating a {Learning} {Health} {System} through {Rapid}-{Cycle}, {Randomized} {Testing}},
	volume = {381},
	issn = {0028-4793, 1533-4406},
	url = {http://www.nejm.org/doi/10.1056/NEJMsb1900856},
	doi = {10.1056/NEJMsb1900856},
	language = {en},
	number = {12},
	urldate = {2022-07-01},
	journal = {New England Journal of Medicine},
	author = {Horwitz, Leora I. and Kuznetsova, Masha and Jones, Simon A.},
	month = sep,
	year = {2019},
	pages = {1175--1179},
}

@article{horwitz2018,
	title = {The {Importance} of {User}-{Centered} {Design} and {Evaluation}: {Systems}-{Level} {Solutions} to {Sharp}-{End} {Problems}},
	volume = {178},
	issn = {2168-6106},
	shorttitle = {The {Importance} of {User}-{Centered} {Design} and {Evaluation}},
	url = {http://archinte.jamanetwork.com/article.aspx?doi=10.1001/jamainternmed.2018.1902},
	doi = {10.1001/jamainternmed.2018.1902},
	language = {en},
	number = {8},
	urldate = {2022-07-01},
	journal = {JAMA Internal Medicine},
	author = {Horwitz, Leora I.},
	month = aug,
	year = {2018},
	pages = {1023},
}

@article{gunning2019,
	title = {{XAI}—{Explainable} artificial intelligence},
	volume = {4},
	number = {37},
	journal = {Science Robotics},
	author = {Gunning, D and Stefik, M and Choi, J and Stumpf, S and Yang, GZ},
	year = {2019},
}

@article{guinney2018,
	title = {Alternative models for sharing confidential biomedical data.},
	volume = {36},
	number = {5},
	journal = {Nature biotechnology},
	author = {Guinney, J and Saez-Rodriguez, J},
	year = {2018},
	pages = {391--392},
}

@misc{jacob2008,
	title = {Mirth: {Standards}-based open source healthcare interface engine},
	url = {http://timreview.ca/article/205},
	urldate = {2022-07-01},
	author = {Jacob, Brauer},
	year = {2008},
}

@article{infant2017,
	title = {Computerised interpretation of fetal heart rate during labour ({INFANT}): a randomised controlled trial.},
	volume = {389},
	number = {10080},
	journal = {Lancet (London, England)},
	author = {INFANT, Collaborative Group},
	year = {2017},
	pages = {1719--1729},
}

@article{hoffman2018,
	title = {Metrics for explainable {AI}: {Challenges} and prospects},
	journal = {arXiv preprint arXiv:1812.04608},
	author = {Hoffman, R and Mueller, S and Klein, G and Litman, J},
	year = {2018},
}

@article{halpern2018,
	title = {Using {Default} {Options} and {Other} {Nudges} to {Improve} {Critical} {Care}:},
	volume = {46},
	issn = {0090-3493},
	shorttitle = {Using {Default} {Options} and {Other} {Nudges} to {Improve} {Critical} {Care}},
	url = {http://journals.lww.com/00003246-201803000-00017},
	doi = {10.1097/CCM.0000000000002898},
	language = {en},
	number = {3},
	urldate = {2022-06-27},
	journal = {Critical Care Medicine},
	author = {Halpern, Scott D.},
	month = mar,
	year = {2018},
	pages = {460--464},
}

@article{hyland2020,
	title = {Early prediction of circulatory failure in the intensive care unit using machine learning},
	volume = {26},
	number = {3},
	journal = {Nature Medicine},
	author = {Hyland, Stephanie L. and Faltys, Martin and Hüser, Matthias and Lyu, Xinrui and Gumbsch, Thomas and Esteban, Cristóbal and Bock, Christian and Horn, Max and Moor, Michael and Rieck, Bastian and Zimmermann, Marc and Bodenham, Dean and Borgwardt, Karsten and Rätsch, Gunnar and Merz, Tobias M.},
	year = {2020},
	pages = {364--373},
}

@article{gu2012,
	title = {Intravenous magnesium prevents atrial fibrillation after coronary artery bypass grafting: a meta-analysis of 7 double-blind, placebo-controlled, randomized clinical trials.},
	volume = {13},
	journal = {Trials},
	author = {Gu, WJ and Wu, ZJ and Wang, PF and Aung, LH and Yin, RX},
	year = {2012},
	pages = {41},
}

@article{ghassemi2021,
	title = {The false hope of current approaches to explainable artificial intelligence in health care},
	volume = {3},
	number = {11},
	journal = {The Lancet. Digital health},
	author = {Ghassemi, M and Oakden-Rayner, L and Beam, AL},
	year = {2021},
	pages = {e745--e750},
}

@article{filos2019,
	title = {A systematic comparison of bayesian deep learning robustness in diabetic retinopathy tasks},
	journal = {arXiv preprint arXiv:1912.10481},
	author = {Filos, A and Farquhar, S and Gomez, AN and Rudner, TJG and Kenton, Z and Smith, L and Alizadeh, M and de Kroon, A and Gal, Y},
	year = {2019},
}

@article{ghoshal2020,
	title = {Estimating uncertainty and interpretability in deep learning for coronavirus ({COVID}-19) detection},
	journal = {arXiv preprint arXiv:2003.10769},
	author = {Ghoshal, B and Tucker, T},
	year = {2020},
}

@article{futoma2020,
	title = {The myth of generalisability in clinical research and machine learning in health care},
	volume = {2},
	issn = {25897500},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2589750020301862},
	doi = {10.1016/S2589-7500(20)30186-2},
	language = {en},
	number = {9},
	urldate = {2022-05-07},
	journal = {The Lancet Digital Health},
	author = {Futoma, Joseph and Simons, Morgan and Panch, Trishan and Doshi-Velez, Finale and Celi, Leo Anthony},
	month = sep,
	year = {2020},
	pages = {e489--e492},
}

@article{feng2021a,
	title = {Selective prediction-set models with coverage guarantees},
	issn = {0006-341X, 1541-0420},
	url = {http://arxiv.org/abs/1906.05473},
	doi = {10.1111/biom.13612},
	abstract = {The current approach to using machine learning (ML) algorithms in healthcare is to either require clinician oversight for every use case or use their predictions without any human oversight. We explore a middle ground that lets ML algorithms abstain from making a prediction to simultaneously improve their reliability and reduce the burden placed on human experts. To this end, we present a general penalized loss minimization framework for training selective prediction-set (SPS) models, which choose to either output a prediction set or abstain. The resulting models abstain when the outcome is diﬃcult to predict accurately, such as on subjects who are too diﬀerent from the training data, and achieve higher accuracy on those they do give predictions for. We then introduce a model-agnostic, statistical inference procedure for the coverage rate of an SPS model that ensembles individual models trained using K-fold cross-validation. We ﬁnd that SPS ensembles attain prediction-set coverage rates closer to the nominal level and have narrower conﬁdence intervals for its marginal coverage rate. We apply our method to train neural networks that abstain more for out-of-sample images on the MNIST digit prediction task and achieve higher predictive accuracy for ICU patients compared to existing approaches.},
	language = {en},
	urldate = {2022-06-24},
	journal = {Biometrics},
	author = {Feng, Jean and Sondhi, Arjun and Perry, Jessica and Simon, Noah},
	month = dec,
	year = {2021},
	note = {arXiv:1906.05473 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {biom.13612},
}

@article{falco2021,
	title = {Governing {AI} safety through independent audits},
	volume = {3},
	number = {7},
	journal = {Nature Machine Intelligence},
	author = {Falco, Gregory and Shneiderman, Ben and Badger, Julia and Carrier, Ryan and Dahbura, Anton and Danks, David and Eling, Martin and Goodloe, Alwyn and Gupta, Jerry and Hart, Christopher},
	year = {2021},
	pages = {566--571},
}

@article{doshi-velez2017,
	title = {Towards a rigorous science of interpretable machine learning},
	journal = {arXiv preprint arXiv:1702.08608},
	author = {Doshi-Velez, F and Kim, B},
	year = {2017},
}

@article{eaneff2020,
	title = {The {Case} for {Algorithmic} {Stewardship} for {Artificial} {Intelligence} and {Machine} {Learning} {Technologies}},
	volume = {324},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.2020.9371},
	doi = {10.1001/jama.2020.9371},
	abstract = {The first manual on hospital administration, published in 1808, described a hospital steward as “an individual who [is] honest and above reproach,” with duties including the purchasing and management of hospital materials. Today, a steward’s job can be seen as ensuring the safe and effective use of clinical resources. The Joint Commission, for instance, requires antimicrobial stewardship programs to support appropriate antimicrobial use, including by monitoring antibiotic prescribing and resistance patterns.A similar approach to “algorithmic stewardship” is now warranted. Algorithms, or computer-implementable instructions to perform specific tasks, are available for clinical use, including complex artificial intelligence (AI) and machine learning (ML) algorithms and simple rule-based algorithms. More than 50 AI/ML algorithms have been cleared by the US Food and Drug Administration for uses that include identifying intracranial hemorrhage from brain computed tomographic scans and detecting seizures in real time. Algorithms are also used to inform clinical operations, such as predicting which patients will “no show” for scheduled appointments. More recently, algorithms that predict in-hospital mortality have been proposed to inform ventilator allocation during the coronavirus disease 2019 pandemic.},
	number = {14},
	urldate = {2022-06-24},
	journal = {JAMA},
	author = {Eaneff, Stephanie and Obermeyer, Ziad and Butte, Atul J.},
	month = oct,
	year = {2020},
	pages = {1397--1398},
}

@article{davis2019,
	title = {A nonparametric updating method to correct clinical prediction model drift.},
	volume = {26},
	number = {12},
	journal = {Journal of the American Medical Informatics Association : JAMIA},
	author = {Davis, SE and Greevy, RA and Fonnesbeck, C and Lasko, TA and Walsh, CG and Matheny, ME},
	year = {2019},
	pages = {1448--1457},
}

@article{connell2019,
	title = {Implementation of a {Digitally} {Enabled} {Care} {Pathway} ({Part} 2): {Qualitative} {Analysis} of {Experiences} of {Health} {Care} {Professionals}},
	volume = {21},
	issn = {1438-8871},
	shorttitle = {Implementation of a {Digitally} {Enabled} {Care} {Pathway} ({Part} 2)},
	url = {http://www.jmir.org/2019/7/e13143/},
	doi = {10.2196/13143},
	abstract = {Background: One reason for the introduction of digital technologies into health care has been to try to improve safety and patient outcomes by providing real-time access to patient data and enhancing communication among health care professionals. However, the adoption of such technologies into clinical pathways has been less examined, and the impacts on users and the broader health system are poorly understood. We sought to address this by studying the impacts of introducing a digitally enabled care pathway for patients with acute kidney injury (AKI) at a tertiary referral hospital in the United Kingdom. A dedicated clinical response team—comprising existing nephrology and patient-at-risk and resuscitation teams—received AKI alerts in real time via Streams, a mobile app. Here, we present a qualitative evaluation of the experiences of users and other health care professionals whose work was affected by the implementation of the care pathway.
Objective: The aim of this study was to qualitatively evaluate the impact of mobile results viewing and automated alerting as part of a digitally enabled care pathway on the working practices of users and their interprofessional relationships.
Methods: A total of 19 semistructured interviews were conducted with members of the AKI response team and clinicians with whom they interacted across the hospital. Interviews were analyzed using inductive and deductive thematic analysis.
Results: The digitally enabled care pathway improved access to patient information and expedited early specialist care. Opportunities were identified for more constructive planning of end-of-life care due to the earlier detection and alerting of deterioration. However, the shift toward early detection also highlighted resource constraints and some clinical uncertainty about the value of intervening at this stage. The real-time availability of information altered communication flows within and between clinical teams and across professional groups.
Conclusions: Digital technologies allow early detection of adverse events and of patients at risk of deterioration, with the potential to improve outcomes. They may also increase the efficiency of health care professionals’ working practices. However, when planning and implementing digital information innovations in health care, the following factors should also be considered: the provision of clinical training to effectively manage early detection, resources to cope with additional workload, support to manage perceived information overload, and the optimization of algorithms to minimize unnecessary alerts.},
	language = {en},
	number = {7},
	urldate = {2022-05-07},
	journal = {Journal of Medical Internet Research},
	author = {Connell, Alistair and Black, Georgia and Montgomery, Hugh and Martin, Peter and Nightingale, Claire and King, Dominic and Karthikesalingam, Alan and Hughes, Cían and Back, Trevor and Ayoub, Kareem and Suleyman, Mustafa and Jones, Gareth and Cross, Jennifer and Stanley, Sarah and Emerson, Mary and Merrick, Charles and Rees, Geraint and Laing, Christopher and Raine, Rosalind},
	month = jul,
	year = {2019},
	pages = {e13143},
}

@article{chen2022a,
	title = {Nudging within learning health systems: next generation decision support to improve cardiovascular care},
	volume = {43},
	issn = {0195-668X, 1522-9645},
	shorttitle = {Nudging within learning health systems},
	url = {https://academic.oup.com/eurheartj/article/43/13/1296/6525340},
	doi = {10.1093/eurheartj/ehac030},
	abstract = {Abstract
            The increasing volume and richness of healthcare data collected during routine clinical practice have not yet translated into significant numbers of actionable insights that have systematically improved patient outcomes. An evidence-practice gap continues to exist in healthcare. We contest that this gap can be reduced by assessing the use of nudge theory as part of clinical decision support systems (CDSS). Deploying nudges to modify clinician behaviour and improve adherence to guideline-directed therapy represents an underused tool in bridging the evidence-practice gap. In conjunction with electronic health records (EHRs) and newer devices including artificial intelligence algorithms that are increasingly integrated within learning health systems, nudges such as CDSS alerts should be iteratively tested for all stakeholders involved in health decision-making: clinicians, researchers, and patients alike. Not only could they improve the implementation of known evidence, but the true value of nudging could lie in areas where traditional randomized controlled trials are lacking, and where clinical equipoise and variation dominate. The opportunity to test CDSS nudge alerts and their ability to standardize behaviour in the face of uncertainty may generate novel insights and improve patient outcomes in areas of clinical practice currently without a robust evidence base.},
	language = {en},
	number = {13},
	urldate = {2022-07-01},
	journal = {European Heart Journal},
	author = {Chen, Yang and Harris, Steve and Rogers, Yvonne and Ahmad, Tariq and Asselbergs, Folkert W.},
	month = mar,
	year = {2022},
	pages = {1296--1306},
}

@article{chen2022,
	title = {Nudging within learning health systems: next generation decision support to improve cardiovascular care.},
	volume = {43},
	number = {13},
	journal = {European heart journal},
	author = {Chen, Y and Harris, S and Rogers, Y and Ahmad, T and Asselbergs, FW},
	year = {2022},
	pages = {1296--1306},
}

@article{collins2015,
	title = {Transparent {Reporting} of a multivariable prediction model for {Individual} {Prognosis} {Or} {Diagnosis} ({TRIPOD}): {The} {TRIPOD} {Statement}},
	volume = {162},
	url = {http://annals.org/article.aspx?doi=10.7326/M14-0697},
	doi = {10.7326/M14-0697},
	number = {1},
	journal = {Annals of Internal Medicine},
	author = {Collins, Gary S and Reitsma, Johannes B and Altman, Douglas G and Moons, Karel G M},
	month = jan,
	year = {2015},
	pages = {55--11},
}

@incollection{breck2019,
	title = {Data validation for machine learning.},
	booktitle = {Data validation for machine learning.},
	author = {Breck, Eric and Polyzotis, Neoklis and Roy, Sudip and Whang, Steven and Zinkevich, Martin},
	year = {2019},
}

@article{braithwaite2020,
	title = {The three numbers you need to know about healthcare: the 60-30-10 challenge},
	volume = {18},
	journal = {BMC medicine},
	author = {Braithwaite, Jeffrey and Glasziou, Paul and Westbrook, Johanna},
	year = {2020},
	pages = {1--8},
}

@article{ben-israel2020,
	title = {The impact of machine learning on patient care: {A} systematic review},
	volume = {103},
	journal = {Artificial Intelligence in Medicine},
	author = {Ben-Israel, David and Jacobs, W. Bradley and Casha, Steve and Lang, Stefan and Ryu, Won Hyung A. and de Lotbiniere-Bassett, Madeleine and Cadotte, David W.},
	year = {2020},
	pages = {101785},
}

@article{amodei2016,
	title = {Concrete problems in {AI} safety},
	doi = {10.48550/arXiv.1606.06565},
	journal = {arXiv preprint arXiv:1606.06565},
	author = {Amodei, D and Olah, C and Steinhardt, J and Christiano, P and Schulman, J and Mané, D},
	year = {2016},
}

@article{abdar2021,
	title = {A review of uncertainty quantification in deep learning: {Techniques}, applications and challenges},
	volume = {76},
	journal = {Information Fusion},
	author = {Abdar, M and Pourpanah, F and Hussain, S and Rezazadegan, D and Liu, L and Ghavamzadeh, M and Fieguth, P and Cao, X and Khosravi, A and Acharya, R and Makarenkov, V and Nahavandi, S},
	year = {2021},
	pages = {243--297},
}

@misc{2021c,
	title = {Data research infrastructure landscape: {A} review of the {UK} data research infrastructure},
	url = {https://dareuk.org.uk/wp-content/uploads/2021/11/DARE_UK_Data_Research_Infrastructure_Landscape_Review_Oct_2021.pdf},
	urldate = {2022-07-01},
	year = {2021},
}

@techreport{2021a,
	type = {Green {Paper}},
	title = {Trusted {Research} {Environments} ({TRE}): {A} strategy to build public trust and meet changing health data science needs},
	url = {https://ukhealthdata.org/wp-content/uploads/2020/07/200723-Alliance-Board_Paper-E_TRE-Green-Paper.pdf},
	number = {version 2.0},
	urldate = {2022-07-01},
	institution = {UK Health Data Research Alliance},
	month = jul,
	year = {2021},
}

@misc{2018,
	title = {Simplified {SQL} projection of {FHIR} resources},
	url = {https://github.com/FHIR/sql-on-fhir/blob/master/sql-on-fhir.md},
	urldate = {2022-07-01},
	year = {2018},
}

@misc{2021b,
	title = {{FHIR} bulk data access (flat {FHIR})},
	url = {https://hl7.org/fhir/uv/bulkdata/index.html},
	urldate = {2022-07-01},
	year = {2021},
}

@techreport{2021,
	title = {Artificial {Intelligence} and {Machine} {Learning} ({AI}/{ML}) {Software} as a {Medical} {Device} {Action} {Plan}},
	url = {https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-software-medical-device?mc_cid=20dc2074ab&mc_eid=c49edc17d2},
	urldate = {2022-07-01},
	institution = {US Food \& Drug Administration},
	month = jan,
	year = {2021},
}

@misc{2013,
	title = {{SEC} {Charges} {Knight} {Capital} {With} {Violations} of {Market} {Access} {Rule}},
	url = {https://www.sec.gov/news/press-release/2013-222},
	urldate = {2022-04-27},
	year = {2013},
}
