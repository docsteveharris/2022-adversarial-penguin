## Introduction
We are surrounded by bold claims and huge investments that suggest that machine learning for health (ML4H) will transform care.[@bunz2021] Proof of principle demonstrations include high impact publications of precision prediction models for shock or acute kidney injury.[@hyland2020; @tomasev2019; @sendak2020] Beyond healthcare, companies such as AirBnB, Facebook, and Uber continue to create enormous commercial value despite owning 'no property, no content and no cars'.[@mcrae2015] Inspired by this, and very much aware of the flaws and unwarranted variation in human decision making[@braithwaite2020], government and industry are now laying heavy bets on these technologies transforming medicine.[@nhsx2022; @2021f]
Digitisation of the health record is prerequisite for this ambition, but adoption is growing at pace.[@ref] Nonetheless ML4H finds itself in Gartner's trough of disillusionment.[@steinert2010] This is unfortunate because the premise that augmenting human decision making with timely and novel insights from data is correct. But ML4H has not learned the lessons that translational medicine delivered to basic laboratory science over the last two decades.[@woolf2008]
ML4H models that have reached the market are embedded in isolated digital workflows (typically radiology) or medical devices.[@muehlematter2021] Here the deployment environment is either fully specified (devices), or static and self-contained (imaging). 
Yet the ambition for ML4H is broader. ML4H should leverage the Electronic Health Record (EHR) too. This requires a strategy to integrate machine learning with the full EHR where data evolves and continuously updates, with multiple interacting users, and eventually interacting algorithms (the machine learning equivalent of poly-pharmacy).[@morse2020a] Hospitals will require an infrastructure that manages algorithms with the same regard that is given to medicines (pharmacy) and machines (medical physics).
Whilst examples of such algorithms exist and are rightly celebrated for the innovation and foresight, they have in reality not left the laboratory bench. The analogy with drug discovery and translational medicine is useful because we need a corresponding professionalisation of ML4H in both _development_ and _deployment_. 
We describe here a living sketch of this future based on our own local experiences in a newly digitally mature institution that has embraced the potential of ML4H. We propose a platform supported by five pillars.
1. Real World Development
2. ML-Ops (DevOps for Machine Learning)
3. Responsible AI
4. Implementation science
5. Continuous clinical evaluation
We describe our local implementation of (1) and (2) here, and sketch the requirements of (3), (4), and (5).

### 1. Real World Development
Real-world data (RW-Data) and Real-world evidence (RW-Evidence) means the use of observational data at scale, and augmented by linking across multiple data sources to generate insights simply not available from isolated controlled clinical trials.[@corrigan-curay2018a] The FDA uses data from tens of millions of patients in its [Sentinel programme](https://www.sentinelinitiative.org) to monitor drug safety, and the [OpenSafely](https://www.opensafely.org) programme in the UK generated impactful insights into COVID-19 within the first few months of the global pandemic.[@williamson2020] 
Given the sensitive nature of health data, these initiatives depend on expanding investment into Trusted Research Environments (TRE). [@2021c] Data flows from source (primary, secondary, social care and elsewhere) to a single secure landing zone where research teams write the code to link, clean and analyse the data. The insights return to the bedside through clinical guidelines and policy. This offline '_data-to-code_' approach is also the dominant design pattern in ML4H projects but is fundamentally flawed.
A (good) data-to-code feedback loop is measured in weeks and months, but ML4H interventions are virtual not physical, and must act by altering the behaviour of clinicians by providing insights for better decisions. As such, perfect information provided to the wrong person, or at the wrong moment, cannot be impactful. Excellent offline model performance provides no guarantee of this bedside efficacy. Algorithms with inferior technical performance may even provide greater bedside utility.[@the2021; @shah2019] 
ML4H requires instead a 'code-to-data' paradigm, and an internal Trusted _Development_ Environment (TDE) _within_ the healthcare institution not externally in an academic facility. This permits live Real World Development (RW-Dev) that includes the end-user in rapid-cycle build-test-learn loops.(TODO: Figure EMAP)
A TDE has three functional requirements that distinguish it from a TRE. (1) Firstly, data updates must match the cadence of clinical decision making. For most inpatient and acute care pathways, decisions are in realtime (minutes or hours) at the bedside or in the clinic. (2) Secondly, privacy must be managed such that teams are able to develop end-user applications that inevitably display patient identifiable information alongside the model outputs: an anonymous prediction is of little use to a clinician. (3) Thirdly, attention must be paid to developer ergonomics. Whether development and deployment are separated physically (the TRE paradigm) or functionally (languages and technologies) then responsibilities divide between different teams. One team prepares the raw data and develops the model, and another prepares the live data and deploys the model. This separation of concerns impedes iteration, and reduces quality.[@dev-ops continuous development ref]
Our TDE is named the Experimental Medicine Application Platform (EMAP). It is clinical laboratory within which ML4H researchers can iteratively build, test and gather feedback from the bedside. 
#### 1. Live data using HL7v2
We used HL7 (version 2.3+) rather than FHIR as our primary means of access to the EHR because the standard is open, the messages are real time, and the infrastructure already exists even in even digitally immature hospitals. A similar approach was used to build a statewide clinical data warehouse in South Carolina spanning multiple institutions and merging data for millions of patients. [@turley2016] 
Within a hospital, rather than multiple direct pair-wise connections between each system, HL7v2 messages are routed through an integration or interface engine.[@jacob2008] Integration engines are a core component in most if not all digital health systems. They orchestrate message routing from source to one or more consumers, they manage the message queues, and because HL7v2 standards are loose they often translate messages to ensure compatibility between systems. 
This is advantageous because it creates a single point of contact and reduces installation and maintenance costs. Moreover, because messages were pushed (rather than pulled) from the live clinical system the work load remains under the control of the source system. Downstream applications are unable to escalate demand to upstream systems. This last point means there is minimal additional burden on clinical systems (the message are already being generated) thus reducing deployment risk.
#### 2. Code-to-data
Live data brings an orthogonal but important secondary benefit in that it inverts the data-to-code paradigm.[@guinney2018] Integrating development and deployment mandates a platform within the same security envelope as the EHR. This code-to-data approach avoids many of the well known challenges of data sharing that troubles health data research.[@powles2017] Instead, developers come to work with data under the same controls and protections as the original record.
We argue that this approach appropriately balances risks and benefits. In particular, whilst there is greater risk delegating responsibility to individuals that is mitigated through the benefits of a responsive development environment. Rather than extracting and attempting to anonymise health data, and moving these data to externally located researchers, we bring the researchers to the data. Enabling 'code to data' rather than relying on 'data to code' allows us to draw on all of the '5 safes': safe data, safe people, safe settings, safe projects, and safe outputs.[ref] 
Both EMAP databases and the development environment are hosted within the hospital ﬁrewall (private cloud). Users are under the same governance process as all hospital staﬀ etc. There is no external access. Authorised developers may work with access to PII (Personally Identiﬁable Information) for application deployment but research teams work with a de-identiﬁed view of the UDS. Deployed applications integrate the PII with the ML4H model and display predictions to clinical teams in context.
####  3. Developer ergonomics
Our third requirement focuses on developer ergonomics. A 'code-to-data' approach is not feasible if the necessary tooling is not available in the health care environment. This means providing parity in the development environment with regards to tools and workflow. 
##### Application Development Environment
Whilst it is possible to deploy a static set of analytic and development tools with relative ease, this does not support a modern data science or application development workflow. Such workflows assume free access to install new tools and update dependencies, to collaborate through version control platforms, and to perform continuous integration testing using external infrastructure. We provide a Linux based virtual machine hosted within the hospital digital environment. The user is not given root access (i.e. full control) but the machine is adapted to support the deployment of applications hosted within docker containers.  Network access, filtered by an HTTP proxy, permits the build and orchestration of docker containers. Version control is available locally through a git server, and appropriately certified users are able to use external version control tools.  Filters automatically exclude files identified as likely to contain data, and private repositories are preferred by default. Applications are able to verify user identity against the hospital's active directory so that web applications can be authenticated using existing credentials.[^1]
##### SQL: unified interface for as wide an audience as possible
Most researchers expect to access data as flat tables, or from a relational database. The learning curve for using FHIR is not steep, but it is not designed for bulk queries nor is it able to handle procedures such as joins or aggregation.[@2018c] Flat (bulk) FHIR is not mature, nor designed for live interactions. [@2021b] This means that training and prediction, and development and deployment would require separate workflows, skills and potentially staffing. We therefore specified a single data source for both training and prediction, and chose SQL as the interface with the widest group of users in health care. Specifically, this means the platform is accessible to business intelligence and audit teams as well as ML4H developers.

### 2. ML-Ops
In ML4H, the data and the algorithm are the 'celebrity couple'. State-of-the-art models trained on RW-Data deliver high impact publications.[Tomasev; Hyland] But only a tiny handful (just 8 studies in a recent high quality systematic review of 1909 ML4H publications[@ben-israel]), are prospectively implemented. The offline data-to-code paradigm described above incurs a significant but 'hidden technical debt' that includes configuration, data collection and verification, feature extraction,  analysis and process tools, machine resource management, serving infrastructure, and monitoring.[@sculley2015] In fact, the code for the underlying ML model is estimated to be at most 5% of the total code with the other 95% representing 'glue code' used to make the system work with generic packages. 'Glue-code', 'pipeline jungles', and 'dead experimental codepaths' are some of the anti-patterns that make the transition into production costly and hazardous.[^2]
Agencies such as the [FDA](https://www.fda.gov), [EMA](https://www.ema.europa.eu/en), and [MHRA](https://www.gov.uk/government/organisations/medicines-and-healthcare-products-regulatory-agency) are working toward safety standards for AI and machine learning, but the majority of these efforts derive from medical devices regulation. Treating Software as a Medical Device (SaMD) is appropriate where the algorithms operate within a constant and predictable environment (e.g. code embedded within a cardiac pacemaker). ML4H models working with the EHR are likely to find themselves operating in a significantly more complex landscape. EHR data feeds will change, new wards will open, staffing patterns will be adjusted, and from time to time major incidents (even global pandemics) will disrupt everything.
Those same tech giants that are generating value from data have responded by adopting an approach to model deployment called 'ML-Ops'. This is the machine learning equivalent of 'DevOps' (a portmanteau of Software Development plus IT operations)[@ref:wikipedia?] that focuses on the quality and speed with which software updates move from concept to production. A typical ML-Ops system monitors raw input data, checks for distribution drift, provides a feature store to avoid train/serve skew and facilitate collaboration between teams, and maintains an auditable and monitored model repository.[@ref] 
Our ML-Ops platform is named FlowEHR: a working prototype supports the deployment and maintenance of a handful of local operational models.[@ref:zella] FlowEHR supports the ingestion of live data from EMAP (above) alongside retrospective data from reporting data warehouses. FHIR will be supported once generally available. We can augment the platform with external data sources (e.g. to provide population level priors or regional or national COVID data). The raw input data is automatically monitored and compared against expected distributions to defend against data drift and ensure first line data quality.
FlowEHR implements an emerging ML architectural pattern known as a _feature store_. Although popularised by hyperscale tech giants, the feature store is not about data volume but rather offers a solution to data consistency and data reuse problems, both of which act as major barriers to collaboration and iterative model improvement. The feature store provides three important capabilities:
1. **Feature generation**: researchers collaborate with developers in a deliberate 'pairing' to implement code to transform data into usable features in a consistent and testable manner for both training and prediction. We argue that it is crucial that the data used in training and prediction have undergone the identical processing steps to avoid difficult performance issues (e.g. train/serve skew).[@ref:train/serve] Generated features are also monitored and provide a second line data quality defence.
2. **Feature storage**: the platform persists the current state of features as well as a record of historical point-in-time correct versions of all features. This is alongside an auditable repository of metadata such as feature versions, data lineage, and usage tracking. The attributes ensure reproducibility, facilitate reuse of common features between teams, and improve safety by providing a mechanism for audit.[@falco2021]
3. **Feature serving**: the platform exposes a consistent API with separate interfaces for serving large batches of historical data for offline training, and vectors of the latest feature values needed for prediction. Providing separate read pathways enables de-identification of offline training data used by researchers while ensuring the live data (containing PII) has undergone the exact same feature transformations. 
FlowEHR extends the emphasis on developer ergonomics by providing an access-controlled notebook-style data science environment based on industry standard open-source tooling for the ML modelling lifecycle. The environment provides a simple and easy way to log experiment runs and artefacts alongside hyper parameter strategies and model versions. A structured workflow provides users with sufficient guidance to ensure reproducibility and avoid 'spaghetti code' common in research teams but dangerous for deployment.[@pizka2004]
In addition to the feature store, FlowEHR provides an auditable model store for those models that graduate to production. These models are deployed in a decoupled manner using modern container technology and web APIs to allow outputs to be consumed by different services. Once again continuous monitoring guards against model drift and guides retraining decisions.[@davis2019]

## 3. AI safety

## 4. Implementation science
## 5. Continuous clinical evaluation

## Continuous clinical evaluation
We have used an analogy with translational medicine to emphasise the need to prioritise an integrated deployment strategy that includes development, safety and implementation. However, the analogy breaks down at the evaluation stage which for drug discovery should result in a randomised controlled clinical trial (RCT). Rigorous evaluation including randomisation is essential but an ML4H model is not the same as the patented and fixed active ingredient in a medication. It is a dynamic tool that will must not only evolve but also may be specific and contextual rather than generalisable. There is no single point in time nor single host environment when it can be declared effective (or otherwise). This distinction is not a dichotomy. The more closely a model is related to a biological process or embedded in a device, the more generalisable and more constant will be its domain. Examples would include ML4H tools deployed for digital pathology, for patient monitoring or within imaging workflows. Of more than 350,000 studies registered on [ClinicalTrials.gov](#) in 2020, just 358 evaluated ML4H, and only 66 were randomised.[@zippel2021] Once again notable examples (imaging, cataract screening, colonoscopy, cardiotocographs etc.) are not interacting with EHR data.[^1] 
But the value of ML4H in decision support will include operational workflows and clinical pathways that do not easily generalise. This is not a lack of external validity and a critique of the potential of the field. An algorithm that is useful or important in one hospital does not have to be relevant or useful in another. It does mean however that institutions deploying and relying on these tools will need to integrate an approach to evaluation of clinical and operational endpoints into the deployment process. They will not be able to rely on external evidence. 
This requirement in turn mandates an alternative to the classical parallel arm RCT that incurs lengthy and expensive governance procedures. Yet this more agile solution cannot be less rigorous or shortcut the tenets of Good Clinical Practice. Thus far, randomisation has been considered too difficult and 'silent-mode' models, or retrospective evaluations or prospective cross-over/sequential implementation strategies are most often used. Given the inherent risk of bias in these observational designs, there will be a need to find a logistically tractable and ethically acceptable method of using randomisation.
One solution that we have piloted is a technique called nudge-randomisation.[@wilson2022a] This is specifically designed to generate learning opportunities for treatment pathways with existing variation in practice. The ethical justification is two-fold: firstly, that patients are exposed to varying treatment regimes by dint of their random interaction with different clinicians based on geography (the healthcare provider they access) and time (staff holidays and shift patterns etc.). This routine variation in practice is summarised as the 60-30-10 problem: 60% of care follows best practice; 30% is wasteful or ineffective and 10% is harmful.[@braithwaite2020] The second ethical justification is that the randomisation is non-mandatory: a nudge not an order. The clinician complies with the randomisation only where they have equipoise themselves, but overrules the randomisation where they have a preference. 
Consider the following worked example. We train an algorithm to optimise electrolyte supplementation in critically ill patients at risk of new onset atrial fibrillation (NOAF). We have already observed significant variation in the propensity of the clinical team to administer supplemental magnesium and potassium. Our algorithm evaluates the risk of NOAF in realtime based on existing electrolyte levels, medications, disease type, and the patient's co-morbidities. We have evidence that such supplementation is impactful for cardiac surgery patients[ref] but no evidence that either our algorithm works nor that the existing evidence generalises to other patient populations. Our model is now used to drive a clinical decision support system (CDSS) that operates in two layers. Firstly, where electrolytes are outside existing (evidence based guidelines) the CDSS makes a strong deterministic recommendation. Secondly, where electrolytes are within the window of the broader guideline but could be optimised, the CDSS makes a nudged randomised recommendation based on the model's prediction.
During the pilot phase the study might run with pre-emptive consent, but if the pilot demonstrates safety and acceptability (for the methodology rather than the intervention) then it justifiable to transition to opt-out consent. This allows the trial to scale, and treatment effects to be estimated as per any RCT with imperfect compliance.[@wilson2022]
Here the nudge is directly linked to the treatment allocation via the CDSS, but randomised indirect nudges without CDSS are also possible.[@chen2022] These might alter how information and choice is presented within the EHR by altering the ordering of investigations in a pick list. Such approaches have already demonstrated efficacy for diagnostic, screening and monitoring test orders.[@main2010]






 





[^1]:	A typical set-up would see the team run a Jupyter Lab server within a docker container on that machine with shared file access to permit collaboration between team members. The ML4H researcher would work from this environment using separately managed credential to access live and static data stores. An application developer would orchestrate dockerised applications as needed to support the researcher. These might include web applications to return the model results directly to the clinical team for evaluation, or a services that would provide a suitable API for the results to be called from the EHR. This delegated responsibility strikes a balance between security, and a responsive development environment. 

[^2]:	One infamous example from the financial services sector saw a firm lose $170,000 per second (\>$400m in 45 minutes) when an outdated piece of code leaked into production. The firm in question was fined a further $12m for 'inadequate safeguards' allowing 'millions of erroneous orders'. [@2013b]