# Work-in-progress

## Introduction
We are surrounded by bold claims and huge investments that suggest that machine learning for health (ML4H) will transform care.[@bunz2021]() Proof of principle demonstrations include high impact publications of precision prediction models for shock or acute kidney injury.[@hyland2020; @tomasev2019; @sendak2020]() Beyond healthcare, companies such as AirBnB, Facebook, and Uber continue to create enormous commercial value despite owning 'no property, no content and no cars'.[@mcrae2015]() Inspired by this, and very much aware of the flaws and unwarranted variation in human decision making[@braithwaite2020](), government and industry are now laying heavy bets on these technologies transforming medicine.[@nhsx2022; @2021f]() 
The lack of a mature digital infrastructure had been the major barrier impeding the application of these technologies, but Electronic Health Record System (EHRS) adoption is growing and that is no longer a valid excuse.
Nonetheless ML4H finds itself in Gartner's trough of disillusionment.[@steinert2010]() This is unfortunate because the premise that augmenting human decision making with timely and novel insights from data is correct. But ML4H has not learned the lessons that translational medicine delivered to basic laboratory science over the last two decades.[@woolf2008]() The ML4H innovations published in high impact journals have in reality not left the laboratory bench. There is no infrastructure to see them deployed and managed by healthcare providers. Instead, these early innovations carry the "hidden technical debt" of machine learning, and hospitals are unable to realise their potential.[@sculley2015]() 
The analogy with drug discovery and translational medicine is useful because we need a corresponding professionalisation of ML4H in both _development_ and _deployment_. Those algorithms that successfully cross Topol and Keane's 'AI Chasm'[@topol2018]() are normally embedded in medical devices or isolated digital workflows (medical imaging). Here the deployment environment is either fully specified (devices), or static and self-contained (imaging). Yet the ambition for Ml4H is broader. This requires a strategy to integrate machine learning with the full EHR where data evolves and continuously updates, with multiple interacting users, and eventually interacting algorithms (the machine learning equivalent of poly-pharmacy).[@morse2020a]() Hospitals will require an infrastructure that manages algorithms with the same regard that is given to medicines (pharmacy) and machines (medical physics).
We describe here a living sketch of this future based on our own local experiences in a newly digitally mature institution that has embraced the potential of ML4H. We propose a platform supported by five pillars.
1. Real world (live) development
2. ML-Ops (DevOps for Machine Learning)
3. Responsible AI
4. Implementation science
5. Continuous clinical evaluation
We describe our local implementation of (1) and (2) here, and sketch the requirements of (3), (4), and (5).
---- 
### 1. Real world (live) development
Real-world data (RWD) and Real-world evidence (RWE) means the use of observational data at scale, and augmented by linking across multiple data sources to generate insights simply not available from isolated controlled clinical trials.[@corrigan-curay2018a]() The FDA uses data from tens of millions of patients in its [Sentinel programme]()(https://www.sentinelinitiative.org) to monitor drug safety, and the [OpenSafely]()(https://www.opensafely.org) programme in the UK generated impactful insights into COVID-19 within the first few months of the global pandemic.[@williamson2020]() 
Given the sensitive nature of health data, these initiatives depend on expanding investment into Trusted Research Environments (TRE). [@2021c]() Data flows from source (primary, secondary, social care and elsewhere) to a single secure landing zone where research teams write the code to link, clean and analyse the data. The insights return to the bedside through clinical guidelines and policy. Although this offline 'data-to-code' approach is also the dominant design pattern in ML4H, it is flawed.
A 'data-to-code' feedback loop is measured in weeks and months, but ML4H interventions are virtual not physical, and must act by altering the behaviour of clinicians by providing insights for better decisions. As such, perfect information provided to the wrong person, or at the wrong moment, cannot be impactful. Excellent offline model performance provides no guarantee of this bedside efficacy. Algorithms with inferior technical performance may even provide greater bedside utility.[@the2021; @shah2019]() 
We propose an alternative 'code-to-data' paradigm, and an internal Trusted _Development_ Environment (TDE) within the healthcare institution instead of an external TRE. A TDE has three functional requirements that distinguish it from a TRE. Firstly, data updates must match the cadence of clinical decision making. For most inpatient and acute care pathways, decisions are in realtime at the bedside or in the clinic. Secondly, privacy must be managed such that teams are able to develop end-user applications that inevitably display patient identifiable information alongside the model outputs. An anonymous prediction is of little use to a clinician. Thirdly, attention must be paid to developer ergonomics. Whether development and deployment are separated physically (the TRE paradigm) or functionally (languages and technologies) then responsibilities split between different teams. This separation of concerns impedes iteration, and reduces quality.[@dev-ops continuous development ref]()
Our TDE is named the Experimental Medicine Application Platform (EMAP). It is clinical laboratory within which ML4H researchers can iteratively build, test and gather feedback from the bedside. 

---- 
### 2. ML-Ops
In ML4H, the data and the algorithm are the celebrity couple. State-of-the-art models trained on RWD deliver high impact publications.[Tomasev; Hyland]() But only a tiny handful (just 8 studies in a recent high quality systematic review of 1909 ML4H publications[@ben-israel]()), are prospectively implemented. The offline data-to-code paradigm described above incurs a significant but 'hidden technical debt' that includes configuration, data collection and verification, feature extraction,  analysis and process tools, machine resource management, serving infrastructure, and monitoring.[@sculley2015]() In fact, the code for the ML model is estimated to be at most 5% of the total code with the other 95% representing 'glue code' used to make the system work with generic packages. 'Glue-code', 'pipeline jungles', and 'dead experimental codepaths' are some of the anti-patterns that make the transition into production costly and hazardous. One infamous example from the financial services sector saw a firm lose $170,000 per second (\>$400m in 45 minutes) when an outdated piece of code leaked into production. The firm in question was fined a further $12m for 'inadequate safeguards' allowing 'millions of erroneous orders'. [@2013b]()
Agencies such as the [FDA]()(https://www.fda.gov), [EMA]()(https://www.ema.europa.eu/en), and [MHRA]()(https://www.gov.uk/government/organisations/medicines-and-healthcare-products-regulatory-agency) are working toward safety standards for AI and machine learning, but the majority of these efforts derive from medical devices regulation. Treating Software as a Medical Device (SaMD) is appropriate where the algorithms operate within a constant and predictable environment (e.g. code embedded within a cardiac pacemaker). ML4H models are likely to find themselves operating in a significantly more complex landscape. EHR data feeds will change, new wards will open, staffing patterns will be adjusted, and from time to time major incidents (even global pandemics) will disrupt everything.
Those same tech giants that are generating value from data have responded by adopting an approach to model deployment called 'ML-Ops'. This is the machine learning equivalent of 'DevOps' (a portmanteau of Software Development plus IT operations)[@ref:wikipedia?]() that focuses on the quality and speed with which software updates move from concept to production. A typical ML-Ops system monitors raw input data, checks for distribution drift, provides a feature store to avoid train/serve skew and facilitate collaboration between teams, and maintains an auditable and monitored model repository.[@ref]() 






 






# Submitted and accepted abstract

We are surrounded by bold claims, and huge investments that suggest that machine learning for health is going to transform care. [@bunz2021]() Predictors of shock or acute kidney injury have been rigorously developed using cutting edge methods from well funded and highly successful teams. [@hyland2020; @tomasev2019]() Governments have deployed enormous resources to realising value from the digitisation of healthcare.[@nhsx2022]() There is also no doubt that health care delivery is imperfect, and human decision making is variable and not always reaching the quality standards we aspire to. [@braithwaite2020]() The lack of a mature digital infrastructure has been a major barrier until recently, but this is no longer a valid excuse especially in well funded academic medical centres. 
ML4H finds itself in Gartner's trough of disillusionment. [@steinert2010]() This is unfortunate because the premise that augmenting human decision making with timely and insights from data is correct. The problem is that ML4H has not learned the lessons that translational medicine delivered to basic science over the last two decades. [@woolf2008]() The ML4H innovations that are published in high impact journals have in reality not left the laboratory bench, and there is no infrastructure to see them deployed and managed by healthcare providers. They carry the full "hidden technical debt" of machine learning.[@sculley2015]() The analogy with drug discovery is useful because we need a corresponding professionalisation of ML4H in both _development_ and _deployment_. 
Algorithms that cross the AI chasm are normally embedded in medical devices or isolated digital workflows (medical imaging). Here the deployment environment is either fully-specified (devices) or static and self-contained (imaging). The ambition for ML4H is broader but requires a strategy to integrate with the full Electronic Health Record (EHR) with multiple users and continuously updating and evolving data. [@vaitla2020a]() Hospitals will require infrastructure that manages algorithms with the same regard that is given to medicines (pharmacy) and machines (medical physics).
We describe here a living sketch of this future based on our own local experiences in a  newly digitally mature institution that has embraced the potential of ML4H. Our outline is for an academic health science centre that sees itself involved in both the development and the deployment of algorithms. 
We argue that the following five pillars are necessary for success, and provide examples of our own work toward each.
### Real world development requires more than real world data
Because ML4H interventions are not drugs or devices that have their own physical existence, the algorithm cannot be effectively developed away from the target environment. Code-to-data rather than data-to-code requires a sandboxed development environment within the hospital. The sandbox protects clinical and operational systems whilst updating data at the same cadence as the work of  the bedside decision-maker. Current solutions are incomplete. Reporting data warehouses can provide data for training but are rarely live. FHIR can serve data for live predictions but not at scale for training, and FHIR implementations are incomplete.[@2018c]() Combining the two places means that teams must maintain separate data pipelines increasing cost and reducing efficiency. Until these two solutions converge, we outline our local implementation built using HL7v2 and SQL-as-an-API philosophy that means features for training and prediction are served from the same pipeline using the same infrastructure and tools. 
### We must embrace and extend the field of ML-Ops.
The model artefact in a real-world ML system is a tiny fraction of the whole. [@sculley2015]() Unlike traditional software artefacts, the quality of an ML model is a reflection of the quality of the underlying data. [@renggli2021]() At organisations with mature ML deployments, such as Netflix, Uber, AirBnB, practises have evolved to ensure high quality data is continuously provided to their models. [@hermann2017]() These have collectively become known as MLOps and includes model and hyper-parameter versioning, continuous monitoring for data and concept drift, iterative model development and fail-safes triggered by model drift. MLOps is quickly becoming an important enabler of successful ML deployment across many industries. However, in the high-stakes healthcare environment, data quality issues are safety issues, and our MLOps  implementation goes further to tackle additional requirements like regulatory approval and information governance.[@sambasivan2021]()
### Responsible AI
Model recommendations always carry unintended consequences since the models only learn from a digital representation of the world that cannot encode moral or ethical standards. Some will be harmful because unmeasured confounders lead to incorrect conclusions (banning matches will reduce smoking related lung cancer).[@jeter2019]() Some will be harmful because the training data is simply incomplete. This is largely responsible for the ethnicity bias in imaging processing.[@ghassemi2021]() Both of these risks will diminish with experience and better digitisation. 
The new challenge will be managing the impact of an AI recommendation on the model's own learning. Models that modify clinician's behaviour alter the data that will go on to update the next iteration of that same model. In some circumstances, 'naive updating' of the model will lead to a convergence where the model just predicts its own effect.[@liley2021]()
### Implementation science
Unlike medications, algorithms have no external existence and can only impact health by influencing the behaviour of clinicians and patients. This corresponds to the second (T2) arm of translational medicine: implementation science.[@woolf2008]() A well designed, safe, and responsible AI algorithm may still be ineffective if it does reach a modifiable target of the clinical pathway.[@the2021]() This field will need a multi-disciplinary approach that involves human-computer interaction, behavioural science, and qualitative analysis.[@sendak2020]() Crucially, it will be most effective when development is done online and in a real-world environment. This means that a handful of key institutions must become design 'laboratories' where rapid prototyping (build-test-learn) at the bedside crafts the deployment pathway for effectiveness rather than just efficacy.    
### Continuous evaluation
We will need to move beyond simple 'silent runs' of ML models to understand their worth. Standalone parallel arm randomised controlled trials may be sometimes necessary but are not sustainable across the breadth of interventions where ML4H might return value. Instead, we should leverage the inherently digital nature of our field to build appropriate and continuous forms of evaluation. Key outcomes can be monitored through the EHRS as the system is implemented, and appropriate causal inference methods brought to bear (e.g. interrupted time series analysis) to estimate treatment effects. In many cases, the algorithm will be operating in a field with existing variation in practice.[@braithwaite2020]() A portion of this variation represents substandard care, but another portion represents genuine clinical equipoise.[@london2018]() This latter should be the target domain of a digital learning health care system that delivers point of care randomisation.[forthcoming]() Algorithimic recommendations can then be evaluated rigorously and efficiently. This approach will require innovations in trial design (e.g. cohort embedded RCTs), patient and public engagement, and models of consent, but these are within our gift.
## Conclusion
Previous work cites issues such as restricted access to data, data quality and clinician distrust of black box algorithms.[@ghassemi2021; @2020]() Others cite the need for new approaches to evaluating ML algorithms. 
Focus on these narrow technical issues obscures deeper issues around the processes, infrastructure and incentives to support healthcare ML research. There are important conceptual shifts that require integration of new expertise into the development team. Failure to recognise these issues leads to unrealistic expectations about the time and resources needed to develop a clinically-useful algorithm and lack of clarity about how to conceptualise and evaluate success. The situation is further exacerbated by digital health evaluation frameworks which give the impression that all the questions of relevance can be answered in parallel.
By laying out a framework for describing the stages of algorithm development, we hope to create a common language that research funders, programme directors and ML researchers can use to support discussions around strategic investment in healthcare ML research and development.
 



