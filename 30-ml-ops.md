# ML-Ops (for health)
Hitherto in ML4H, the data and the algorithm have been the 'celebrity couple'. State-of-the-art models trained on RW-Data deliver high profile publications.[@tomasev2019; @hyland2020] But only a tiny handful (just 8 studies in a recent high quality systematic review of 1909 ML4H publications[@ben-israel2020]), are prospectively implemented. The standard offline 'data-to-modeler' (DTM)  paradigm described above incurs a significant but 'hidden technical debt' that includes configuration, data collection and verification, feature extraction,  analysis and process tools, compute and storage resource management, serving infrastructure, and monitoring.[@sculley2015] In fact, the code for the underlying ML model is estimated to be at most 5\% of the total code with the other 95\% used to make the system work with generic packages. 'Glue-code', 'pipeline jungles', and 'dead experimental codepaths' are some of the anti-patterns that make the transition into production costly and hazardous.[^1]

Agencies such as the [FDA](https://www.fda.gov), [EMA](https://www.ema.europa.eu/en), and [MHRA](https://www.gov.uk/government/organisations/medicines-and-healthcare-products-regulatory-agency) are working toward safety standards for AI and machine learning, but the majority of these efforts derive from medical devices regulation. Treating Software as a Medical Device (SaMD) is appropriate where the algorithms operate within a constant and predictable environment (e.g. code embedded within a cardiac pacemaker). But, as already argued, ML4H models working with the EHR are likely to find themselves operating in a significantly more complex landscape. This inconstant environment where algorithms themselves may only have temporary utility has parallels to the commercial environment exploited so successfully by the tech giants. 

These companies have cultivated an approach to model deployment called 'ML-Ops'. This combines the practices of 'DevOps' (a portmanteau of Software Development plus IT operations)[@2022b] that focuses on the quality and speed with which software updates move from concept to production, with robust data engineering and machine learning. A typical ML-Ops system monitors raw input data, checks for distribution drift, provides a feature store to avoid train/serve skew and facilitate collaboration between teams, and maintains an auditable and monitored model repository.[@john2021] We present a prototype implementation interacting with the EHRS in Figure 2 (called FlowEHR).

This constant adjustment of algorithm based on their continuously measured quality and performance needs a workforce as well as a technology stack. Just as the safe delivery of medicines to the bedside is the central activity of a hospital pharmacy team, the safe delivery of algorithms will require the development of similarly skilled and specialised practitioners, and we should expect to see clinical ML-Ops departments in the hospital of the future. Others have made similar proposals and labelled this as "algorithmic stewardship" or "AI-QI".[@eaneff2020;@feng2022] 

[^1]:	One infamous example from the financial services sector saw a firm lose \$170,000 per second (more than \$400m in 45 minutes) when an outdated piece of code leaked into production. The firm in question was fined a further \$12m for "inadequate safeguards" allowing "millions of erroneous orders".[@2013]