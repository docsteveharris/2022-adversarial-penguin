The value of ML4H in practice depends not only on the successful delivery of patient benefit but also on successful mitigation of any unintentional harm that AI technologies may induce.  Model recommendations always carry unintended consequences as they only learn from a digital representation of the world that cannot encode moral or ethical standards. Unfair outcomes, discrimination in sub-populations and biased decisions [@amodei2016] are just a few examples of AI shortcomings discussed in literature. In a dynamic setting, risk can also arise in the form of degraded predictive performance over time. Models that modify clinician's behaviour alter future patient profiles by design. But as predictive algorithms operate on patterns of historical data, its success today can inherently erode its performance in the future by rendering obsolete the historical patterns it had originally learned from. These are just a few examples of the challenges that risk management systems for AI safety must face. Responsible and safe AI in practice requires a systems-approach that pre-empts and safe-guards against potential risks to patients. In confronting AI-technology risks in ML4H systems, we highlight three promising frameworks to consider when developing a platform of robust enterprise-wide controls.

We argue that at the model selection stage, model explainability needs to be prioritised as one of the key metrics.  Most AI models are not designed with explainability constraints and operate as ‘black-box models’. On a practical level, ‘black-box models’ are unsuitable for mission-critical domains, such as in healthcare, because they pose risk scenarios where problems that occur can remain masked and therefore undetectable and unfixable. Explainable AI research [@gunning2019],[@mueller2019],[@vilone2020],[@Linardatos2020] on methods to highlight decision-relevant parts of AI representations and for measuring and benchmarking interpretability [@Doshi-Velez2017],[@Hoffman2018] are particularly promising for risk management as they can be used to structure a systematic interrogation of the trade-off between interpretability, model accuracy and the risk of model misbehaviour.

Another important aspect to consider is the inclusion of fail-safe support systems to pre-empt and mitigate model misbehaviour. The European Commission High-Level Expert Group on AI presented the Ethics Guidelines for Trustworthy Artificial Intelligence in April 2019 with recommendations for AI-support systems that continue to maintain human-agency via a human-in-the-loop oversight. Prediction models that map patient data to medically meaningful classes are forced to predict within the predetermined set of classes without the option to flag users when the model is unsure of an answer. To address this problem, there is good evidence that methods such as Bayesian deep learning and various uncertainty estimates [@abdar2021] can provide promising ways to detect and refer data samples with high probability of misprediction for human expert review [@Leibig2017],[@Filos2019],[@Ghoshal2020]. With this , when fully interpretable models are unavailable, less interpretable models may become acceptable options when implemented in conjunction with an effective fail-safe system.

The last aspect concerns the development of risk protocols to manage the dynamic model calibration. As discussed, models that influence the evolution of its own future input data are at risk of performance deterioration over time due to input data shifts. On the other hand, models can generate biased decisions as a result of the inherent biases found within the dataset that the model was built from. In both cases, continual learning via model recalibration is required. However, continual learning remains a challenging paradigm in AI; Recalibration with  non-stationary incremental data can lead to catastrophic forgetting when the new data negatively interferes with what the model has already learned [@Parisi2019]. A potential solution is to fully retrain the model instead of recalibrating whenever new batches of data become available. However, with information healthcare governance constantly evolving, there are serious barriers to the adoption of this approach [@Lee2020]. Technically, new solutions to assess the quality of continual learning models will also be important.