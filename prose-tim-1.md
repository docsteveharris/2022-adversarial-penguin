# Introduction

Extrapolating from the impacts on other industries such as hospitality and transportation,[@mcrae2015] many experts predict that machine learning (ML) will transform healthcare.[@bunz2021] (ref: https://qualitysafety.bmj.com/content/28/3/231) Thus far the ML models that have reached the market are embedded in medical devices or isolated digital workflows, typically involving processing of medical images.[@muehlematter2021] However, when it comes to ML models operating on data held in an electronic health record system (EHRS), few models have progressed beyond academic prototypes and none have found widespread adoption, despite significant investment from government bodies [@nhsx2022] and commercial companies [@2021f] in the field.

Previous work cites issues such as restricted access to data, data quality and clinician distrust of black box algorithms.[@ghassemi2021; @2020] Others cite the need for new approaches to evaluating ML algorithms.  Focus on these narrow technical issues obscures deeper issues around the processes, infrastructure and incentives to support healthcare ML research.

The recently published Goldacre report sets out a number of recommendations around how healthcare data can be better utilised in the UK National Health Service. A key recommendation is the development and widespread adoption of standardised Trusted Research Environments (TREs), secure computing environments that provide remote access to health data for approved researchers to use in research. The FDA uses data from tens of millions of patients in its [Sentinel programme](https://www.sentinelinitiative.org) to monitor drug safety. In the UK, the [OpenSafely](https://www.opensafely.org) programme generated impactful insights into COVID-19 within the first few months of the global pandemic.[@williamson2020]

While TREs excel at meeting the needs of population health scientists, they do not have the full complement of features required to take a machine learning algorithm from bench to bedside, especially where the ML algorithm is operating on EHRS data. In this paper we describe the process of ML-based Clinical Decision Support System (CDSS) development, derive the key requirements of a TDE  to support this process and describe the architecture a prototype TDE implemented within our institution.

# Section 1: ML-based Clinical Decision Support System (CDSS) development process

We describe the process of CDSS development by comparing and contrasting with the process of drug development.

## Pre-Clinical Phase

In pre-clinical phase of drug development the objective is to identify candidate molecules which might make effective drugs. Evaluation is conducted in vitro. Metrics used to evaluate candidates, such as binding affinity or other pharmacokinetic properties, describe the properties of the molecule. (ref: [https://www.sciencedirect.com/science/article/pii/S1359644617304695](https://www.sciencedirect.com/science/article/pii/S1359644617304695) ).

In the pre-clinical phase of CDSS development the objective is to identify candidate algorithms, comprising of input variables and model structures, which might make the core of an effective CDSS. Evaluation is conducted offline on de-identified datasets. Metrics used to evaluate candidates, such Area Under the Receiver Operator Curve (AUROC), the F1 score and calibration, describe the properties of the algorithm.(ref TRIPOD)

In addition to investigating model performance the development dataset must also be evaluated to identify biases within the dataset which may affect model output. Beyond conducting statistical analyses of the dataset, researchers need to be aware of the processes by which the data are generated. Understanding the process of data generation may lead to identification of issues that are not readily apparent from analysis of the data alone. (Ref: [https://www.nature.com/articles/s41591-019-0548-6](https://www.nature.com/articles/s41591-019-0548-6)) In our experience, this process is greatly facilitated by having the research located within the hospital, thereby lowering the barrier to visiting the wards where data are collected to observe data entry and discuss the real-world documentation practices with clinical taff.

## Phase 1 Trials

Phase 1 drug trials are the first time a drug candidate is tested in humans. They are conducted in small numbers of healthy volunteers. The aim of the trial is to determine the feasibility of progressing to trials in patients by determining drug safety and appropriate dosage. Drug formulation, the processes by which substances are combined with the active pharmaceutical ingredient to optimise the acceptability and effective delivery of the drug, is also considered at this stage.

Phase 1 CDSS trials are the first time an algorithm candidate is tested within the hospital environment. The aim of the trial is to determine the feasibility of progressing to trials in the clinical environment by ensuring the algorithm implementation is safe, reliable and able to cope with real-world data quality issues. The development of a mechanism to deliver of algorithm outputs embedded in the clinical workflow is also be considered at this stage. Multiple systematic reviews have highlighted that the key determinant of CDSS success is whether the decision support is embedded within the clinical workflow. (ref: [https://www.bmj.com/content/330/7494/765.short](https://www.bmj.com/content/330/7494/765.short) , [https://www.nature.com/articles/s41746-020-0221-y](https://www.nature.com/articles/s41746-020-0221-y))

The implementation of a Phase 1 ML4H trial represents a significant undertaking. Live deployment requires four key components, a live data pipeline delivering data from the hospital EHRS, de-identification of data on the fly to protect patient privacy, a user-interface for displaying algorithm outputs which can be embedded within the clinical workflow and “MLOps” systems to ensure the safe, consistent and reliable execution of the algorithm and monitoring of input data quality. [@sculley2015]

In addition to the technical complexity of implementation [^2], real-world implementation is commonly impeded by the need to obtain buy-in from hospital IT teams and comply with the change-control processes designing to minimise risk to the IT infrastructure. In practice these challenges mean that few research teams manage to successfully complete Phase 1 ML4H trials.

## Phase 2 Trials

Phase 2 drug trials involve recruitment of small numbers patients with the disease of interest, typically 50 – 200. [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6609997/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6609997/) The aim is to determine drug efficacy at treating the disease. Treating clinicians are involved in so far as they must agree to prescribe the drug for their patients. The trials are often too short to determine long term outcomes, therefore surrogate measures such biomarker status or change in tumour size are used as endpoints. (ref: [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6609997/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6609997/))

Phase 2 ML4H trials involve recruitment of small numbers of clinicians making the decision of interest, typically 5 – 10. (ref usability man) The aim is to determine the efficacy of the algorithm in improving their decisions. Patients are involved in so far as they must agree to be on the receiving end of these supported decisions and identifiable data is required. Endpoints are markers of successful task completion in all cases. 50-68% of studies additionally report patient outcomes. ([https://jamanetwork.com/journals/jama/fullarticle/200503](https://jamanetwork.com/journals/jama/fullarticle/200503))

In addition to reporting user performance and patient outcomes, investigations to determine ways in which the system could be more successful in influencing user behaviour are carried out at this stage. These include usability analyses, considerations of how well the CDSS is integrated into the overall system and implementation studies to identify how best to optimise end-user adoption and engagement. [https://www.nature.com/articles/s41746-020-0221-y](https://www.nature.com/articles/s41746-020-0221-y) Data collection for such studies can be greatly enhanced by the use of usability assessment tools embedded within the user interface. These include embedded forms for end-users to report issues and user interface monitoring.  (ref: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7779159)

## Phase 3 Studies

Phase 3 drug trials involve the recruitment of large numbers of patients to determine whether a drug is effective in improving patient outcomes. The gold standard of trial design is a double-blinded randomised controlled trial (RCT).

The optimal approach to generate evidence for CDSS tool is a matter of ongoing debate. Of more than 350,000 studies registered on [ClinicalTrials.gov](#) in 2020, just 358 evaluated ML4H, and only 66 were randomised.[@zippel2021] The majority of algorithms which make it to this stage are not interacting with EHR data but rather imaging or sensor data.[@infant2017; @titano2018; @wang2019; @wu2019; @lin2019a; @turakhia2019a; @long2017]

Rigorous evaluation including randomisation is essential but the RCT paradigm assumes a controlled intervention deployed in a stable, widely generalised context. This applies to ML algorithms operating on imaging, sensor or laboratory test data [@muehlematter2021] but does not apply to data generated as a by-product of operational workflows and clinical pathways, which are often institution specific. An algorithm that is useful or important in one hospital does not have to be relevant or useful in another: the 'myth of generalisability'.[@futoma2020] It does mean however that institutions deploying and relying on these tools will need to integrate an approach to evaluation of local clinical and operational endpoints into the deployment process. They will not be able to rely on external evidence.

This requirement in turn mandates an alternative to the classical parallel arm RCT that incurs lengthy and expensive governance procedures. Yet this more agile solution cannot be less rigorous or shortcut research ethics. Thus far, randomisation has been considered too difficult and 'silent-mode' models, or retrospective evaluations or prospective cross-over/sequential implementation strategies are most often used. Given the inherent risk of bias in these observational designs, there will be a need to find a logistically tractable and ethically acceptable method of using randomisation.

One solution that we have piloted is a technique called nudge-randomisation. [@wilson2022a] This is specifically designed to generate learning opportunities for treatment pathways with existing variation in practice. The ethical justification is two-fold: firstly, that patients are exposed to varying treatment regimes by dint of their random interaction with different clinicians based on geography (the healthcare provider they access) and time (staff holidays and shift patterns etc.). This routine variation in practice is summarised as the 60-30-10 problem: 60% of care follows best practice; 30% is wasteful or ineffective and 10% is harmful.[@braithwaite2020] The second ethical justification is that the randomisation is non-mandatory: a nudge not an order. The clinician complies with the randomisation only where they have equipoise themselves, but overrules the randomisation where they have a preference.

During a pilot phase a nudge trial might run with pre-emptive consent, but if the pilot demonstrates safety and acceptability (for the methodology rather than the intervention) then it justifiable to transition to opt-out consent. This allows the trial to scale, and treatment effects to be estimated as per any RCT with imperfect compliance.[@wilson2022] The feasibility of conducting such trials will be greatly increased by the incorporation of tools to aid randomisation and recruitment into the front-end application.

## Discussion

As we have shown, there are considerable parallels between the intellectual complexity scientific concerns of the drug and algorithm development. However, ML-based CDSS development is further complicated by the fact that at each stage not only does the performance of the algorithm need to be monitored but also the properties of the input data.

The process of conducting drug trials is supported by a investment in infrastructure embedded within hospitals, including clinical research facilities, pharmacy services for trial drug handling, clinical trials units and well understood approvals processes. Despite this drug development takes 10-15 years (ref: [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6113160/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6113160/) ).  By contrast, even in the most advanced centres (ref Yale, Sendak)  only a fraction of the equivalent infrastructure exists to support ML4H trials.

We estimate the code for an ML model and user interface to be at most 5% of the total code required for its live deployment. In the absence of a shared development environment research groups operating independently within an academic health centre have to bear the cost of developing the other 95% of the code. Not only is this wasteful but the majority of research groups will not have the expertise to do so effectively. Until this problem is addressed the potential of ML4H is unlikely to be realised at scale.

Based on our description of the ML-based CDSS development process we derive the following functional requirements that distinguish a TDE from a TRE. Firstly, there must be a live data pipeline out of the EHRS. Live data ensures algorithm output is always based on the most recent data available at the time of a clinical decision making. Secondly, privacy must be managed such that teams are able to develop end-user applications that inevitably display patient identifiable information alongside the model outputs: an anonymous prediction is of little use to a clinician. Thirdly, MLOps systems must be present to ensure algorithm outputs are reliable. Fourthly, there must be a mechanism for seamlessly embedding model outputs within the clinical workflow in the EHRS. For some projects a mechanism as simple as an inbound messaging interface to trigger alerts using the native functionality of an EHRS might suffice. In other cases it will be necessary to host a custom user interface within the EHRS. Finally, it should provide functions to support clinical evaluation, ranging from usability studies to randomisation for large scale trials.

While the similarities with drug development are useful to highlight some aspects of the development process, we do not advocate that the same method of development, whereby each phase of development must be completed before proceeding to the next one, be applied for the development of CDSSs. For software development, incremental or iterative approaches have been found to result in greater developer productivity and lower costs than a sequential “waterfall” approach (Ref: [https://www.cin.ufpe.br/~in1037/AllFinal/SE52%20Mitchell%202009.pdf](https://www.cin.ufpe.br/~in1037/AllFinal/SE52%20Mitchell%202009.pdf)) and result in lower quality software. [@2022b] Additionally, excellent offline model performance provides no guarantee of bedside efficacy. Algorithms with inferior technical performance may even provide greater bedside utility.[@the2021; @shah2019] This motivates early deployment of candidate ML algorithms to evaluate their impact of user behaviour to guide further refinements. A key enabler of rapid-cycle build-test-learn loops is the situation of the TDE _within_ the healthcare institution rather than externally in an academic facility.[@guinney2018]

# Section 2: Implementation

Having described the process of CDSS development, and the key features of a TDE, we describe the design of our local implementation, the Experimental Medicine Application Platform (EMAP). EMAP is an early stage TDE and, as such, does not currently implement all the features described above. It is hosted within the hospital and provides a live data pipeline to our hospital’s Epic EHRS, inline deidentification of patient data, an MLOps features and a both development and analytics environments. The architecture diagram is shown in (Fig 1)

## Live data pipeline

We opted to use HL7v2 as the core of our live data pipeline on the basis that HL7 messaging is implemented by all EHR vendors and this would aid the generalisation of our design to other institutions. A similar approach was used to build a statewide clinical data warehouse in South Carolina spanning multiple institutions and merging data for millions of patients.[@turley2016]

Within our institution HL7 messages are routinely used to share patient demographics, admission, discharge and transfer events, radiology reports, laboratory test results, blood transfusion orders, and outpatient appointment scheduling. A copy of these messages is sent to an HL7 message log, the Immutable Data Store, implemented as a table in a Postgres database. The raw message text is stored alongside simple metadata to aid searching and filtering of messages. New messages in the IDS are parsed and the resulting information is stored in a second Postgres database, the User Data Store. Providing access to live data via a SQL database allows data scientists who are not software developers to rapidly develop application prototypes using tools which are familiar to them, such as Tableau, R Shiny or Plotly Dash.

The HL7 message interfaces described above are supplemented custom interface to access “flowsheet” data live. Flowsheets are used for the majority of recurrently charted data as well as some fields of structured clerking documents. The flowsheets data sent is determined by adjusting the interface configuration. Rather than attempting to transmit all flowsheet data, we have opted to incrementally add flowsheets to the interface based on end-user request.

Use of HL7 advantageous because messages are pushed by the live clinical system, downstream applications are unable to escalate demand and thus unlikely to impact the performance of the source systems. The predominant disadvantage of HL7 is that not all information from the EHRS can be made available via an HL7 interface.

In order to make other important data live, most notably medications, Epic’s inbuilt live reporting function, Reporting Workbench, is used to poll the live system every 15 minutes and retrieve any newly updated data. This approach has the advantage of being generalisable to any data held within Epic. The trade-off is that each query increases load on the live system and data latency is increased to 15 minutes.

In addition to the HL7 and Reporting Workbench pipelines, a ETL pipeline is used to bring in data which changes infrequently from the trust data warehouse, Caboodle, and other non-Epic systems. We plan to investigate how the Epic FHIR interface and Web APIs might be used to supplement or replace the existing pipelines in the future.

## MLOps

The ML-Ops subsystem, FlowEHR, supports the deployment and maintenance of a handful of local operational models.[@King2022.03.07.22271999] Input data from the EMAP datastore or externally loaded datasets are automatically monitored and compared against expected distributions to defend against data drift and ensure first line data quality.

FlowEHR also implements an emerging ML architectural pattern known as a _feature store_. The feature store provides three important capabilities:

1. **Feature generation**: our researchers and developers collaborate in a deliberate 'pairing' to implement code to transform data into usable features in a consistent and testable manner for both training and prediction. We argue that it is crucial for data used in training and prediction to have undergone the identical processing steps to avoid difficult to diagnose performance issues (e.g. train/serve skew).[@breck2019] Generated features are also monitored which provides a second line data quality defence.

2. **Feature storage**: the platform persists the current state of features as well as a record of historical point-in-time correct versions of all features. This is alongside an auditable repository of metadata such as feature versions, data lineage, and usage tracking. These attributes ensure reproducibility, facilitate reuse of common features between teams, and improve safety by providing a mechanism for audit.[@falco2021]

3. **Feature serving**: the platform exposes a consistent API with separate interfaces for serving large batches of historical data for offline training, and vectors of the latest feature values needed for prediction. Providing separate read pathways enables dynamic de-identification of offline training data used by our researchers while ensuring the live data (containing PII) has undergone the exact same feature transformations.

In addition to the feature store, FlowEHR provides an auditable model store for those models that graduate to production. These models are deployed in a decoupled manner using modern container technology and web APIs to allow outputs to be consumed by different services. Once again continuous monitoring guards against model drift and guides retraining decisions.[@davis2019]

## Development environment

Users can interact with data stored in the EMAP datastores using standard analytical tools provided on a virtual Windows desktop. This provides an welcoming environment for less technically able users.

In addition to the Windows desktop, users can request access to a Linux based virtual machines, configured to support the deployment of applications hosted within docker containers.  Network access, filtered by an HTTP proxy, permits the build and orchestration of docker containers. Version control is available locally through a git server, and appropriately certified users are able to use external version control tools.  Filters automatically exclude files identified as likely to contain data, and private repositories are preferred by default. Applications are able to verify user identity against the hospital's active directory so that web applications can be authenticated using existing credentials.[^1]

FlowEHR extends the emphasis on developer ergonomics by providing an access-controlled notebook-style data science environment based on industry standard open-source tooling for the ML modelling lifecycle. The environment provides a simple and easy way to log experiment runs and artefacts alongside hyper parameter strategies and model versions. A structured workflow provides users with sufficient guidance to ensure reproducibility and avoid 'spaghetti code' common in research teams but dangerous for deployment. [@pizka2004]

# Limitations

In this paper we have focused on the architecture of the data pipeline and the analytics environment. A key component of the pipeline which we have not discussed in detail is privacy-preserving technologies to allow widespread data sharing and minimisation of the need for the use of patient identifying data in the early stages of development. The use of such technologies needs to be supported by local and national information governance and research ethics policies. (ref goldacre) This is a rapidly evolving area and we anticipate that a consensus on best practices will emerge in the next few years.

A further aspect of TDE design that we have not discussed is infrastructure design to facilitate a route to market. In order to have impact beyond the institution where they were developed, ML-based CDSSes must be easily commercialisable. Computable contracts (ref: https://journals.sagepub.com/doi/full/10.1177/20555636211072560) to support data sharing agreements, intellectual property allocation as part of multi-organisation collaborations have the potential to facilitate collaborations with industry partners, thereby easing the route to market. However, at this stage they remain experimental.

# Conclusion

In this paper we have described the process of ML-based CDSS development and presented the argument that TDEs, purpose-built infrastructure designed to support rapid cycles of iterative development, are essential for scalable development and to minimise wastage of research funding. We have also presented an early stage implementation.

By laying out a framework for describing the stages of algorithm development, we hope to create a common language that research funders, programme directors and ML researchers can use to support discussions around strategic investment in healthcare ML research and development.

Further research is required to identify the best architectural design patterns for TDEs and how best to incorporate technologies to enhance data privacy and accelerate widespread adoption.