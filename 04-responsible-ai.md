# Responsible AI in practice
Pillars 1 (real world development) and 2 (ML-Ops) should engender well designed and well engineered algorithms, but they do not protect against the unintentional harm that AI may induce. Algorithms can only learn from a digital representation of the world that cannot encode moral or ethical standards. Unfair outcomes, discrimination against subpopulations and bias are all reported shortcomings.[@amodei2016]  In a dynamic setting, risk can also arise in the form of degraded predictive performance over time. Models that modify clinician's behaviour alter patient profiles by design, but predictive success today inevitably erodes future performance by rendering obsolete the historical patterns that drove the performance of the original model.[@liley2021] Responsible AI in practice requires a systems approach that pre-empts and safe-guards against these potential risks to patients. We highlight three promising responses to components of this challenge that need to become part of the risk management approach for ML4H.

## Model explainability
At the model selection stage, model explainability needs to be prioritised as one of the key metrics.  Most AI models are not designed with explainability constraints and operate as ‘black-box models’. On a practical level, ‘black-box models’ are unsuitable for healthcare because they pose risk scenarios where problems that occur can remain masked and therefore undetectable and unfixable. Explainable AI research [@gunning2019; @mueller2019; @vilone2020; @Linardatos2020] provides methods to highlight decision-relevant parts of AI representations and to measure and benchmarking interpretability. [@Doshi-Velez2017; @Hoffman2018] This is necessary for designing  risk management as it enables a systematic interrogation of the trade-off between interpretability, model accuracy and the risk of model misbehaviour.

## Model fail-safes
Fail safes should be designed into support systems to pre-empt and mitigate model misbehaviour. The European Commission High-Level Expert Group on AI presented the Ethics Guidelines for Trustworthy Artificial Intelligence in April 2019 with recommendations for AI-support systems that continue to maintain human-agency via a human-in-the-loop oversight. Prediction models that map patient data to medically meaningful classes are forced to predict within the predetermined set of classes without the option to flag users when the model is unsure of an answer. To address this problem, there is good evidence that methods such as Bayesian deep learning and various uncertainty estimates [@abdar2021] can provide promising ways to detect and refer data samples with high probability of misprediction for human expert review [@Leibig2017; @Filos2019; @Ghoshal2020]. This may even permit less interpretable models to operate when implemented in conjunction with an effective fail-safe system.

## Dynamic model calibration
As discussed, models that influence the evolution of its own future input data are at risk of performance deterioration over time due to input data shifts. On the other hand, models can generate biased decisions as a result of the inherent biases found within the dataset that the model was built from. In both cases, continual learning via model recalibration is required but continual learning remains a challenging paradigm in AI. Recalibration with  non-stationary incremental data can lead to catastrophic forgetting when the new data negatively interferes with what the model has already learned[@Parisi2019], or a convergence where the model just predicts its own effect.[@liley2021] Here Pillar 1 (real-world development) with suitable audit and monitoring via Pillar 2 (ML-Ops) will be required to overcome what would otherwise be a learning process encumbered by regulatory barriers.[@Lee2020]