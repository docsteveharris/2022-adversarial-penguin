# Responsible AI in practice
Pillars 1 and  2 should engender well designed and well engineered algorithms, but they do not protect against the unintentional harm that AI may induce. Algorithms can only learn from a digital representation of the world that representation in turn cannot encode moral or ethical standards. Unfair outcomes, discrimination against sub-populations and bias are all reported shortcomings.[@amodei2016]  In a dynamic setting, risk can also arise in the form of degraded predictive performance over time. Models that modify clinician's behavior alter patient profiles by design, but predictive success today inevitably erodes future performance by rendering obsolete the historical patterns that drove the performance of the original model.[@liley2021] Responsible AI in practice requires a systems approach that preempts and safe-guards against these potential risks to patients. We highlight three promising responses to components of this challenge that need to become part of the risk management approach for ML4H.

## Model explainability
We argue that model explainability (Explainable Artificial Intelligence [XAI]) methods need to be prioritized to help systematize and coordinate the processes of model troubleshooting by developers, risk-management by service providers, and system-checks by auditors.[@gunning2019;@mueller2019;@vilone2020;@linardatos2020] Most AI models that operate as ‘black-box models’ are unsuitable for mission-critical domains, such as healthcare, because they pose risk scenarios where problems that occur can remain masked and therefore undetectable and unfixable. We acknowledge recent critiques[@ghassemi2021;@krishna2022a] of explainability methods that argue the methods cannot yet be relied on to provide a determinate answer as to whether an AI-recommendation is correct. However, these methods do highlight decision-relevant parts of AI representations, and offer promise in measuring and benchmarking interpretability[@doshi-velez2017;@hoffman2018]. They are particularly promising for risk management as they can be used to structure a systematic interrogation of the trade-off between interpretability, model accuracy and the risk of model misbehavior.

## Model fail-safes
Prediction models that map patient data to medically meaningful classes are forced to predict without the option to flag users when the model is unsure of an answer. To address this problem, there is good evidence that methods such as Bayesian deep learning and various uncertainty estimates [@abdar2021] can provide promising ways to detect and refer data samples with high probability of misprediction for human expert review.[@leibig2017; @filos2019; @ghoshal2020] These fail safes, or selective prediction approaches should be designed into support systems to preempt and mitigate model misbehavior.[@chow1970;@bartlett2008; @tortorella2000;@elyaniv2010;@feng2021] Of note, the European Commission High-Level Expert Group on AI presented guidelines for trustworthy AI in April 2019 with such recommendations: for systems that continue to maintain human-agency via a human-in-the-loop oversight. This may even permit less interpretable models to operate when implemented in conjunction with an effective fail-safe system.

## Dynamic model calibration
As discussed, models that influence the evolution of its own future input data are at risk of performance deterioration over time due to input data shifts [@davis2017]. In such cases, continual learning via calibration drift detection and model recalibration [@feng2022;@davis2020] provides a promising solution but remains a challenging paradigm in AI. Recalibration with non-stationary incremental data can lead to catastrophic forgetting when the new data negatively interferes with what the model has already learned [@parisi2019], or a convergence where the model just predicts its own effect and thus should not be updated [@liley2021]. On the other hand, models can propose poor decisions because of the inherent biases found within the original dataset. In this case, dynamic model recalibration is unlikely to be sufficient and larger model revisions may be required. Here Pillar 1 (RW-dev) with suitable audit and monitoring via Pillar 2 (ML-Ops) will be required to overcome what would otherwise be a learning process encumbered by regulatory barriers.[@lee2020] 
