# Responsible AI in practice
Pillars 1 and  2 should engender well designed and well engineered algorithms, but they do not protect against the unintentional harm that AI may induce. Algorithms can only learn from a digital representation of the world that in turn cannot encode moral or ethical standards. Unfair outcomes, discrimination against sub populations and bias are all reported shortcomings.[@amodei2016]  In a dynamic setting, risk can also arise in the form of degraded predictive performance over time. Models that modify clinician's behaviour alter patient profiles by design, but predictive success today inevitably erodes future performance by rendering obsolete the historical patterns that drove the performance of the original model.[@liley2021] Responsible AI in practice requires a systems approach that pre-empts and safe-guards against these potential risks to patients. We highlight three promising responses to components of this challenge that need to become part of the risk management approach for ML4H.

## Model explainability
We argue that model explainability methods [@gunning2019],[@mueller2019],[@vilone2020],[@Linardatos2020] need to be prioritised to help systematise and coordinate the processes of model troubleshooting by developers, risk-management by AI-backed service provider and system-checks by auditors.  Most AI models that operate as ‘black-box models’ are unsuitable for mission-critical domains, such as healthcare, because they pose risk scenarios where problems that occur can remain masked and therefore undetectable and unfixable. As highlighted in [@Ghassemi 2021],[@Satyapriya2022] explainability methods cannot yet be relied on to provide a determinate answer as to whether an AI-recommendation is correct. However, explainability methods that highlight decision-relevant parts of AI representations and for measuring and benchmarking interpretability [@Doshi-Velez2017],[@Hoffman2018] are particularly promising for risk management as they can be used to structure a systematic interrogation of the trade-off between interpretability, model accuracy and the risk of model misbehaviour.


## Model fail-safes
Fail safes, otherwise referred to as selective prediction [@Chow1970],[@ Bartlett 2008],[@Tortorella2000],[@ElYaniv2010],[@Feng2021] should be designed into support systems to pre-empt and mitigate model misbehaviour. The European Commission High-Level Expert Group on AI presented the Ethics Guidelines for Trustworthy Artificial Intelligence in April 2019 with recommendations for AI-support systems that continue to maintain human-agency via a human-in-the-loop oversight. Prediction models that map patient data to medically meaningful classes are forced to predict within the predetermined set of classes without the option to flag users when the model is unsure of an answer. To address this problem, there is good evidence that methods such as Bayesian deep learning and various uncertainty estimates [@abdar2021] can provide promising ways to detect and refer data samples with high probability of misprediction for human expert review [@Leibig2017; @Filos2019; @Ghoshal2020]. This may even permit less interpretable models to operate when implemented in conjunction with an effective fail-safe system. 

## Dynamic model calibration
As discussed, models that influence the evolution of its own future input data are at risk of performance deterioration over time due to input data shifts [@Davis2017]. In such cases, continual learning via calibration drift detection and model recalibration [@Davis2020],[@Feng2022] provides a promising solution but remains a challenging paradigm in AI. Recalibration with non-stationary incremental data can lead to catastrophic forgetting when the new data negatively interferes with what the model has already learned [@Parisi2019], or a convergence where the model just predicts its own effect and thus should not be updated [@liley2021]. On the other hand, models can propose poor decisions because of the inherent biases found within the original dataset. In this case, dynamic model recalibration is unlikely to be sufficient and larger model revisions may be required. Here Pillar 1 (RW-dev) with suitable audit and monitoring via Pillar 2 (ML-Ops) will be required to overcome what would otherwise be a learning process encumbered by regulatory barriers.[@LEE2020] 
